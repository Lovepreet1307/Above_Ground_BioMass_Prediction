{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d839ce-f6b1-4e02-827b-edb2a79222fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RF with RF feature selection...\n",
      "  → HPO for RF with RF using top-10 features\n",
      "  → HPO for RF with RF using top-70 features\n",
      "  → HPO for RF with RF using top-103 features\n",
      "\n",
      "Evaluating XGB with RF feature selection...\n",
      "  → HPO for XGB with RF using top-10 features\n",
      "  → HPO for XGB with RF using top-70 features\n",
      "  → HPO for XGB with RF using top-72 features\n",
      "\n",
      "Evaluating RF with RFE_RF feature selection...\n",
      "  → HPO for RF with RFE_RF using top-10 features\n",
      "  → HPO for RF with RFE_RF using top-70 features\n",
      "  → HPO for RF with RFE_RF using top-74 features\n",
      "\n",
      "Evaluating XGB with RFE_RF feature selection...\n",
      "  → HPO for XGB with RFE_RF using top-10 features\n",
      "  → HPO for XGB with RFE_RF using top-42 features\n",
      "\n",
      "Evaluating RF with XGB feature selection...\n",
      "  → HPO for RF with XGB using top-10 features\n",
      "  → HPO for RF with XGB using top-70 features\n",
      "  → HPO for RF with XGB using top-130 features\n",
      "  → HPO for RF with XGB using top-185 features\n",
      "\n",
      "Evaluating XGB with XGB feature selection...\n",
      "  → HPO for XGB with XGB using top-10 features\n",
      "  → HPO for XGB with XGB using top-49 features\n",
      "\n",
      "Evaluating RF with RF_XGB feature selection...\n",
      "  → HPO for RF with RF_XGB using top-10 features\n",
      "  → HPO for RF with RF_XGB using top-70 features\n",
      "  → HPO for RF with RF_XGB using top-111 features\n",
      "\n",
      "Evaluating XGB with RF_XGB feature selection...\n",
      "  → HPO for XGB with RF_XGB using top-10 features\n",
      "  → HPO for XGB with RF_XGB using top-20 features\n",
      "\n",
      " Saved fold-wise metrics to: metrics/PRF_LiDAR_702/fold_wise/stage1_fold_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "#Stage 1\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create dataset-specific folder name\n",
    "dataset_slug = re.sub(r'\\W+', '_', os.path.splitext(os.path.basename(CONFIG[\"dataset_path\"]))[0])\n",
    "\n",
    "# Define dataset-specific output directories\n",
    "base_dirs = {\n",
    "    \"predictions\": os.path.join(CONFIG[\"predictions_dir\"], dataset_slug),\n",
    "    \"models\": os.path.join(\"models\", dataset_slug),\n",
    "    \"features\": os.path.join(\"features\", dataset_slug),\n",
    "    \"metrics\": os.path.join(\"metrics\", dataset_slug),\n",
    "    \"hyperparams\": os.path.join(\"hyperparams\", dataset_slug),\n",
    "    \"fold_metrics\": os.path.join(\"metrics\", dataset_slug, \"fold_wise\")  # NEW\n",
    "}\n",
    "\n",
    "# Create all output directories\n",
    "for path in base_dirs.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y,\n",
    "                            test_size=CONFIG[\"test_size\"],\n",
    "                            random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    rankings = {}\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                                random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_scores = rf.feature_importances_\n",
    "    rankings['RF'] = list(X_train.columns[np.argsort(rf_scores)[::-1]])\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "              n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_scores = rfe.ranking_\n",
    "    rankings['RFE_RF'] = list(X_train.columns[np.argsort(rfe_scores)])\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "                       random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_scores = xgb.feature_importances_\n",
    "    rankings['XGB'] = list(X_train.columns[np.argsort(xgb_scores)[::-1]])\n",
    "\n",
    "    hybrid_scores = (rf_scores + xgb_scores) / 2\n",
    "    rankings['RF_XGB'] = list(X_train.columns[np.argsort(hybrid_scores)[::-1]])\n",
    "\n",
    "    return rankings\n",
    "\n",
    "def get_model_config(model_name):\n",
    "    if model_name == 'RF':\n",
    "        model_class = RandomForestRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    elif model_name == 'XGB':\n",
    "        model_class = XGBRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    return model_class, param_grid\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    scoring = {\n",
    "        'r2': 'r2',\n",
    "        'mae': 'neg_mean_absolute_error',\n",
    "        'mse': 'neg_mean_squared_error'\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        cv=CONFIG[\"cv_folds\"],\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        refit=\"r2\"\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    ind = np.argmax(grid_search.cv_results_[\"mean_test_r2\"])\n",
    "    r2 = grid_search.cv_results_[\"mean_test_r2\"][ind]\n",
    "    mae = -1 * grid_search.cv_results_['mean_test_mae'][ind]\n",
    "    mse = -1 * grid_search.cv_results_[\"mean_test_mse\"][ind]\n",
    "    return {\n",
    "        \"best_model\": grid_search.best_estimator_,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"r2\": r2,\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae\n",
    "    }\n",
    "\n",
    "def run_pipeline():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    all_results = []\n",
    "    all_fold_metrics = []  \n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    cv_train_preds = pd.DataFrame({'true': y_train.reset_index(drop=True)})\n",
    "\n",
    "    for fs_name, features in feature_rankings.items():\n",
    "        for model_name, Model in [('RF', RandomForestRegressor), ('XGB', XGBRegressor)]:\n",
    "            print(f\"\\nEvaluating {model_name} with {fs_name} feature selection...\")\n",
    "\n",
    "            best_k = None\n",
    "            best_cv_r2 = -np.inf\n",
    "\n",
    "            # Step 1: Find best k using CV\n",
    "            for k in range(1, len(features) + 1):\n",
    "                k_features = features[:k]\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                avg_r2 = np.mean(fold_r2)\n",
    "\n",
    "                if avg_r2 > best_cv_r2:\n",
    "                    best_cv_r2 = avg_r2\n",
    "                    best_k = k\n",
    "\n",
    "            # Step 2: HPO for spaced subset sizes\n",
    "            subset_k_start = CONFIG.get(\"subset_k_start\", 5)\n",
    "            subset_k_step = CONFIG.get(\"subset_k_step\", 5)\n",
    "            subset_ks = [k for k in range(subset_k_start, best_k + 1, subset_k_step)]\n",
    "            if best_k not in subset_ks:\n",
    "                subset_ks.append(best_k)\n",
    "\n",
    "            for k in subset_ks:\n",
    "                k_features = features[:k]\n",
    "                fold_preds = np.zeros(len(y_train))\n",
    "                fold_r2 = []\n",
    "                fold_mse = []\n",
    "                fold_mae = []\n",
    "                \n",
    "                # Track per-fold metrics\n",
    "                for fold_num, (train_idx, val_idx) in enumerate(KFold(CONFIG[\"cv_folds\"]).split(X_train)):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_preds[val_idx] = preds\n",
    "                    \n",
    "                    # Calculate metrics for this fold\n",
    "                    r2 = r2_score(y_train.iloc[val_idx], preds)\n",
    "                    mse = mean_squared_error(y_train.iloc[val_idx], preds)\n",
    "                    mae = mean_absolute_error(y_train.iloc[val_idx], preds)\n",
    "                    \n",
    "                    fold_r2.append(r2)\n",
    "                    fold_mse.append(mse)\n",
    "                    fold_mae.append(mae)\n",
    "                    \n",
    "                    # Store fold-level results\n",
    "                    all_fold_metrics.append({\n",
    "                        'FS_Method': fs_name,\n",
    "                        'Model': model_name,\n",
    "                        'Features_Used': k,\n",
    "                        'Fold': fold_num + 1,\n",
    "                        'R2': r2,\n",
    "                        'MSE': mse,\n",
    "                        'MAE': mae\n",
    "                    })\n",
    "\n",
    "                cv_r2 = np.mean(fold_r2)\n",
    "                cv_mse = np.mean(fold_mse)\n",
    "                cv_mae = np.mean(fold_mae)\n",
    "                \n",
    "                # Calculate variance/std\n",
    "                cv_r2_std = np.std(fold_r2)\n",
    "                cv_mse_std = np.std(fold_mse)\n",
    "                cv_mae_std = np.std(fold_mae)\n",
    "\n",
    "                model_class, param_grid = get_model_config(model_name)\n",
    "                print(f\"  → HPO for {model_name} with {fs_name} using top-{k} features\")\n",
    "                hpo_results = tune_hyperparameters(\n",
    "                    model_class(random_state=CONFIG[\"random_state\"]),\n",
    "                    param_grid,\n",
    "                    X_train[k_features],\n",
    "                    y_train\n",
    "                )\n",
    "\n",
    "                test_preds = hpo_results[\"best_model\"].predict(X_test[k_features])\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                test_mse = mean_squared_error(y_test, test_preds)\n",
    "                test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "                model_id = f\"{fs_name}_{model_name}_Top{k}\"\n",
    "                combined_preds[f'pred_{model_id}'] = test_preds\n",
    "                cv_train_preds[f'pred_{model_id}'] = fold_preds\n",
    "\n",
    "                joblib.dump(hpo_results[\"best_model\"], os.path.join(base_dirs[\"models\"], f\"Model_{model_id}.pkl\"))\n",
    "\n",
    "                with open(os.path.join(base_dirs[\"features\"], f\"Features_{model_id}.json\"), 'w') as f:\n",
    "                    json.dump(k_features, f)\n",
    "\n",
    "                with open(os.path.join(base_dirs[\"hyperparams\"], f\"BestParams_{model_id}.json\"), 'w') as f:\n",
    "                    json.dump(hpo_results[\"best_params\"], f)\n",
    "\n",
    "                all_results.append({\n",
    "                    'FS_Method': fs_name,\n",
    "                    'Model': model_name,\n",
    "                    'Features_Used': k,\n",
    "                    'Best_Params': hpo_results[\"best_params\"],\n",
    "                    'CV_R2': cv_r2,\n",
    "                    'CV_R2_Std': cv_r2_std,  \n",
    "                    'CV_MSE': cv_mse,\n",
    "                    'CV_MSE_Std': cv_mse_std,  \n",
    "                    'CV_MAE': cv_mae,\n",
    "                    'CV_MAE_Std': cv_mae_std, \n",
    "                    'Test_R2': test_r2,\n",
    "                    'Test_MSE': test_mse,\n",
    "                    'Test_MAE': test_mae\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_results)\n",
    "    metrics_df['R2_Rank'] = metrics_df['Test_R2'].rank(ascending=False, method='min')\n",
    "    metrics_df = metrics_df.sort_values('R2_Rank')\n",
    "    metrics_df.to_csv(os.path.join(base_dirs[\"metrics\"], CONFIG['results_csv_path']), index=False)\n",
    "    \n",
    "    # Save fold-wise metrics\n",
    "    fold_metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "    fold_metrics_df.to_csv(os.path.join(base_dirs[\"fold_metrics\"], \"stage1_fold_metrics.csv\"), index=False)\n",
    "    print(f\"\\n Saved fold-wise metrics to: {base_dirs['fold_metrics']}/stage1_fold_metrics.csv\")\n",
    "    \n",
    "    combined_preds.to_csv(os.path.join(base_dirs[\"predictions\"], \"all_predictions.csv\"), index=False)\n",
    "    cv_train_preds.to_csv(os.path.join(base_dirs[\"predictions\"], \"cv_train_predictions.csv\"), index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be003c1-9db5-4d0a-8807-6a67d0bab8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: META-MODEL ENSEMBLE WITH CROSS-VALIDATION\n",
      "Dataset: PRF_LiDAR_702\n",
      "================================================================================\n",
      "\n",
      "Loaded Data:\n",
      "  Training samples: 224\n",
      "  Test samples: 25\n",
      "  Base models available: 22\n",
      "  Stage 1 configurations: 22\n",
      "\n",
      "Top-K values to evaluate: [10, 20, 22]\n",
      "\n",
      "Using 10-fold cross-validation\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ENSEMBLE METHODS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Top-K = 10 Models\n",
      "================================================================================\n",
      "Selected 10 top models based on Stage 1 CV performance\n",
      "\n",
      "[Method 1] Random Averaging (k=10)\n",
      "  CV R² = 0.7005 (±0.1487)\n",
      "  Test R² = 0.8544\n",
      "\n",
      "[Method 2] Top-K Averaging (k=10)\n",
      "  CV R² = 0.7099 (±0.1505)\n",
      "  Test R² = 0.8552\n",
      "\n",
      "[Method 3] Feature Selection + Linear Regression (k=10)\n",
      "\n",
      "  Evaluating: RF + LinearRegression\n",
      "    CV R² = 0.7075 (±0.1214), Test R² = 0.8301 (n=2)\n",
      "\n",
      "  Evaluating: RFE_RF + LinearRegression\n",
      "    CV R² = 0.7051 (±0.1327), Test R² = 0.8094 (n=7)\n",
      "\n",
      "  Evaluating: XGB + LinearRegression\n",
      "    CV R² = 0.7023 (±0.1373), Test R² = 0.8540 (n=7)\n",
      "\n",
      "  Evaluating: RF_XGB + LinearRegression\n",
      "    CV R² = 0.7027 (±0.1340), Test R² = 0.8077 (n=8)\n",
      "\n",
      "================================================================================\n",
      "Top-K = 20 Models\n",
      "================================================================================\n",
      "Selected 20 top models based on Stage 1 CV performance\n",
      "\n",
      "[Method 1] Random Averaging (k=20)\n",
      "  CV R² = 0.7052 (±0.1477)\n",
      "  Test R² = 0.8536\n",
      "\n",
      "[Method 2] Top-K Averaging (k=20)\n",
      "  CV R² = 0.7064 (±0.1468)\n",
      "  Test R² = 0.8522\n",
      "\n",
      "[Method 3] Feature Selection + Linear Regression (k=20)\n",
      "\n",
      "  Evaluating: RF + LinearRegression\n",
      "    CV R² = 0.7050 (±0.1248), Test R² = 0.8293 (n=3)\n",
      "\n",
      "  Evaluating: RFE_RF + LinearRegression\n",
      "    CV R² = 0.6958 (±0.1363), Test R² = 0.8232 (n=7)\n",
      "\n",
      "  Evaluating: XGB + LinearRegression\n",
      "    CV R² = 0.6912 (±0.1499), Test R² = 0.8502 (n=6)\n",
      "\n",
      "  Evaluating: RF_XGB + LinearRegression\n",
      "    CV R² = 0.6952 (±0.1430), Test R² = 0.8389 (n=8)\n",
      "\n",
      "================================================================================\n",
      "Top-K = 22 Models\n",
      "================================================================================\n",
      "Selected 22 top models based on Stage 1 CV performance\n",
      "\n",
      "[Method 1] Random Averaging (k=22)\n",
      "  CV R² = 0.7047 (±0.1462)\n",
      "  Test R² = 0.8546\n",
      "\n",
      "[Method 2] Top-K Averaging (k=22)\n",
      "  CV R² = 0.7047 (±0.1462)\n",
      "  Test R² = 0.8546\n",
      "\n",
      "[Method 3] Feature Selection + Linear Regression (k=22)\n",
      "\n",
      "  Evaluating: RF + LinearRegression\n",
      "    CV R² = 0.6940 (±0.1372), Test R² = 0.8208 (n=5)\n",
      "\n",
      "  Evaluating: RFE_RF + LinearRegression\n",
      "    CV R² = 0.6918 (±0.1339), Test R² = 0.8232 (n=7)\n",
      "\n",
      "  Evaluating: XGB + LinearRegression\n",
      "    CV R² = 0.6906 (±0.1472), Test R² = 0.8502 (n=6)\n",
      "\n",
      "  Evaluating: RF_XGB + LinearRegression\n",
      "    CV R² = 0.6924 (±0.1443), Test R² = 0.8380 (n=7)\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      " Saved aggregated results: metrics/PRF_LiDAR_702/best_meta_model_results.csv\n",
      " Saved fold-wise metrics: metrics/PRF_LiDAR_702/fold_wise/stage2_fold_metrics.csv\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total configurations evaluated: 18\n",
      "\n",
      "Top 5 Configurations by Test R²:\n",
      "       Method  Top_K       Meta_Model FS_Method  Best_n    CV_R2  CV_R2_Std  Test_R2\n",
      "  TopKAverage     10             None      None      10 0.709894   0.150510 0.855246\n",
      "  TopKAverage     22             None      None      22 0.704676   0.146240 0.854591\n",
      "RandomAverage     22             None      None      22 0.704676   0.146240 0.854591\n",
      "RandomAverage     10             None      None      10 0.700518   0.148665 0.854372\n",
      " FS_MetaModel     10 LinearRegression       XGB       7 0.702290   0.137328 0.854024\n",
      "RandomAverage     20             None      None      20 0.705243   0.147656 0.853632\n",
      "  TopKAverage     20             None      None      20 0.706445   0.146830 0.852168\n",
      " FS_MetaModel     20 LinearRegression       XGB       6 0.691211   0.149949 0.850244\n",
      " FS_MetaModel     22 LinearRegression       XGB       6 0.690647   0.147202 0.850244\n",
      " FS_MetaModel     20 LinearRegression    RF_XGB       8 0.695241   0.143008 0.838852\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Stage 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ---------------------\n",
    "# Load config & setup output\n",
    "# ---------------------\n",
    "with open(\"config.json\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "dataset_slug = re.sub(r'\\W+', '_', os.path.splitext(os.path.basename(CONFIG[\"dataset_path\"]))[0])\n",
    "output_dir = os.path.join(\"metrics\", dataset_slug)\n",
    "fold_metrics_dir = os.path.join(output_dir, \"fold_wise\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(fold_metrics_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"STAGE 2: META-MODEL ENSEMBLE WITH CROSS-VALIDATION\")\n",
    "print(f\"Dataset: {dataset_slug}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ---------------------\n",
    "# Load prediction data\n",
    "# ---------------------\n",
    "cv_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/{dataset_slug}/cv_train_predictions.csv\")\n",
    "test_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/{dataset_slug}/all_predictions.csv\")\n",
    "metrics_df = pd.read_csv(f\"metrics/{dataset_slug}/{CONFIG['results_csv_path']}\")\n",
    "\n",
    "X_train = cv_df.drop(columns=[\"true\"])\n",
    "y_train = cv_df[\"true\"]\n",
    "X_test = test_df.drop(columns=[\"true\"])\n",
    "y_test = test_df[\"true\"]\n",
    "model_columns = X_train.columns.tolist()\n",
    "\n",
    "print(f\"\\nLoaded Data:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Base models available: {len(model_columns)}\")\n",
    "print(f\"  Stage 1 configurations: {len(metrics_df)}\")\n",
    "\n",
    "# ---------------------\n",
    "# Feature Selection Methods\n",
    "# ---------------------\n",
    "def fs_rf(X, y, n):\n",
    "    \"\"\"Random Forest feature importance\"\"\"\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    scores = rf.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rfe_rf(X, y, n):\n",
    "    \"\"\"Recursive Feature Elimination with Random Forest\"\"\"\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=42, n_jobs=-1), \n",
    "              n_features_to_select=n)\n",
    "    rfe.fit(X, y)\n",
    "    return X.columns[rfe.support_]\n",
    "\n",
    "def fs_xgb(X, y, n):\n",
    "    \"\"\"XGBoost feature importance\"\"\"\n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb.fit(X, y)\n",
    "    scores = xgb.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rf_xgb(X, y, n):\n",
    "    \"\"\"Hybrid: Average of RF and XGB importances\"\"\"\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    xgb.fit(X, y)\n",
    "    avg_scores = (rf.feature_importances_ + xgb.feature_importances_) / 2\n",
    "    return X.columns[np.argsort(avg_scores)[::-1][:n]]\n",
    "\n",
    "fs_methods = {\n",
    "    \"RF\": fs_rf,\n",
    "    \"RFE_RF\": fs_rfe_rf,\n",
    "    \"XGB\": fs_xgb,\n",
    "    \"RF_XGB\": fs_rf_xgb\n",
    "}\n",
    "\n",
    "# Meta-models\n",
    "def get_meta_model(model_name):\n",
    "    \"\"\"Get a fresh instance of meta-model\"\"\"\n",
    "    if model_name == \"LinearRegression\":\n",
    "        return LinearRegression()\n",
    "    elif model_name == \"RandomForest\":\n",
    "        return RandomForestRegressor(random_state=CONFIG[\"random_state\"], n_jobs=-1)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        return XGBRegressor(random_state=CONFIG[\"random_state\"], n_jobs=-1)\n",
    "\n",
    "meta_model_names = [\"LinearRegression\"]  # Only train LR for Method 3\n",
    "\n",
    "# ---------------------\n",
    "# Configure Top-K values\n",
    "# ---------------------\n",
    "max_models = len(metrics_df)\n",
    "step = CONFIG.get(\"stage2_topk_step\", 10)  # Default to 10 if not in config\n",
    "k_values = list(range(10, max_models, step))\n",
    "if max_models not in k_values:\n",
    "    k_values.append(max_models)\n",
    "\n",
    "print(f\"\\nTop-K values to evaluate: {k_values}\")\n",
    "\n",
    "# Storage for results\n",
    "best_results = []\n",
    "all_fold_metrics = []\n",
    "\n",
    "# Setup CV\n",
    "kfold = KFold(n_splits=CONFIG[\"cv_folds\"], shuffle=True, random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "print(f\"\\nUsing {CONFIG['cv_folds']}-fold cross-validation\")\n",
    "\n",
    "# ---------------------\n",
    "# Main Loop: Evaluate Methods\n",
    "# ---------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ENSEMBLE METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Top-K = {k} Models\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get top-k models based on CV_R2 from Stage 1\n",
    "    top_k_models = metrics_df.sort_values(\"CV_R2\", ascending=False).head(k)\n",
    "    top_k_cols = [\n",
    "        f\"pred_{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "        for _, row in top_k_models.iterrows()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Selected {len(top_k_cols)} top models based on Stage 1 CV performance\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 1: Random Averaging with CV\n",
    "    # ---------------------\n",
    "    print(f\"\\n[Method 1] Random Averaging (k={k})\")\n",
    "    \n",
    "    random.seed(CONFIG[\"random_state\"])\n",
    "    random_cols = random.sample(model_columns, min(k, len(model_columns)))\n",
    "    \n",
    "    fold_metrics_random = []\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        y_pred_val = X_train.iloc[val_idx][random_cols].mean(axis=1)\n",
    "        \n",
    "        r2 = r2_score(y_train.iloc[val_idx], y_pred_val)\n",
    "        mse = mean_squared_error(y_train.iloc[val_idx], y_pred_val)\n",
    "        mae = mean_absolute_error(y_train.iloc[val_idx], y_pred_val)\n",
    "        \n",
    "        fold_metrics_random.append({'R2': r2, 'MSE': mse, 'MAE': mae})\n",
    "        \n",
    "        all_fold_metrics.append({\n",
    "            'Method': 'RandomAverage',\n",
    "            'Top_K': k,\n",
    "            'Meta_Model': 'None',\n",
    "            'FS_Method': 'None',\n",
    "            'Best_n': k,\n",
    "            'Fold': fold_num + 1,\n",
    "            'R2': r2,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae\n",
    "        })\n",
    "    \n",
    "    cv_r2_random = np.mean([f['R2'] for f in fold_metrics_random])\n",
    "    cv_r2_std_random = np.std([f['R2'] for f in fold_metrics_random])\n",
    "    cv_mse_random = np.mean([f['MSE'] for f in fold_metrics_random])\n",
    "    cv_mae_random = np.mean([f['MAE'] for f in fold_metrics_random])\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred_test_random = X_test[random_cols].mean(axis=1)\n",
    "    test_r2_random = r2_score(y_test, y_pred_test_random)\n",
    "    test_mse_random = mean_squared_error(y_test, y_pred_test_random)\n",
    "    test_mae_random = mean_absolute_error(y_test, y_pred_test_random)\n",
    "    \n",
    "    print(f\"  CV R² = {cv_r2_random:.4f} (±{cv_r2_std_random:.4f})\")\n",
    "    print(f\"  Test R² = {test_r2_random:.4f}\")\n",
    "\n",
    "    best_results.append({\n",
    "        \"Dataset\": dataset_slug,\n",
    "        \"Method\": \"RandomAverage\",\n",
    "        \"Top_K\": k,\n",
    "        \"Meta_Model\": \"None\",\n",
    "        \"FS_Method\": \"None\",\n",
    "        \"Best_n\": k,\n",
    "        \"CV_R2\": cv_r2_random,\n",
    "        \"CV_R2_Std\": cv_r2_std_random,\n",
    "        \"CV_MSE\": cv_mse_random,\n",
    "        \"CV_MSE_Std\": np.std([f['MSE'] for f in fold_metrics_random]),\n",
    "        \"CV_MAE\": cv_mae_random,\n",
    "        \"CV_MAE_Std\": np.std([f['MAE'] for f in fold_metrics_random]),\n",
    "        \"Test_R2\": test_r2_random,\n",
    "        \"Test_MSE\": test_mse_random,\n",
    "        \"Test_MAE\": test_mae_random,\n",
    "        \"Selected_Features\": ','.join(random_cols[:5]) + '...' if len(random_cols) > 5 else ','.join(random_cols)\n",
    "    })\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 2: Top-K Averaging with CV\n",
    "    # ---------------------\n",
    "    print(f\"\\n[Method 2] Top-K Averaging (k={k})\")\n",
    "    \n",
    "    fold_metrics_top = []\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        y_pred_val = X_train.iloc[val_idx][top_k_cols].mean(axis=1)\n",
    "        \n",
    "        r2 = r2_score(y_train.iloc[val_idx], y_pred_val)\n",
    "        mse = mean_squared_error(y_train.iloc[val_idx], y_pred_val)\n",
    "        mae = mean_absolute_error(y_train.iloc[val_idx], y_pred_val)\n",
    "        \n",
    "        fold_metrics_top.append({'R2': r2, 'MSE': mse, 'MAE': mae})\n",
    "        \n",
    "        all_fold_metrics.append({\n",
    "            'Method': 'TopKAverage',\n",
    "            'Top_K': k,\n",
    "            'Meta_Model': 'None',\n",
    "            'FS_Method': 'None',\n",
    "            'Best_n': k,\n",
    "            'Fold': fold_num + 1,\n",
    "            'R2': r2,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae\n",
    "        })\n",
    "    \n",
    "    cv_r2_top = np.mean([f['R2'] for f in fold_metrics_top])\n",
    "    cv_r2_std_top = np.std([f['R2'] for f in fold_metrics_top])\n",
    "    cv_mse_top = np.mean([f['MSE'] for f in fold_metrics_top])\n",
    "    cv_mae_top = np.mean([f['MAE'] for f in fold_metrics_top])\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred_test_top = X_test[top_k_cols].mean(axis=1)\n",
    "    test_r2_top = r2_score(y_test, y_pred_test_top)\n",
    "    test_mse_top = mean_squared_error(y_test, y_pred_test_top)\n",
    "    test_mae_top = mean_absolute_error(y_test, y_pred_test_top)\n",
    "    \n",
    "    print(f\"  CV R² = {cv_r2_top:.4f} (±{cv_r2_std_top:.4f})\")\n",
    "    print(f\"  Test R² = {test_r2_top:.4f}\")\n",
    "\n",
    "    best_results.append({\n",
    "        \"Dataset\": dataset_slug,\n",
    "        \"Method\": \"TopKAverage\",\n",
    "        \"Top_K\": k,\n",
    "        \"Meta_Model\": \"None\",\n",
    "        \"FS_Method\": \"None\",\n",
    "        \"Best_n\": k,\n",
    "        \"CV_R2\": cv_r2_top,\n",
    "        \"CV_R2_Std\": cv_r2_std_top,\n",
    "        \"CV_MSE\": cv_mse_top,\n",
    "        \"CV_MSE_Std\": np.std([f['MSE'] for f in fold_metrics_top]),\n",
    "        \"CV_MAE\": cv_mae_top,\n",
    "        \"CV_MAE_Std\": np.std([f['MAE'] for f in fold_metrics_top]),\n",
    "        \"Test_R2\": test_r2_top,\n",
    "        \"Test_MSE\": test_mse_top,\n",
    "        \"Test_MAE\": test_mae_top,\n",
    "        \"Selected_Features\": ','.join(top_k_cols[:5]) + '...' if len(top_k_cols) > 5 else ','.join(top_k_cols)\n",
    "    })\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 3: FS + Linear Regression Meta-Model with Nested CV\n",
    "    # ---------------------\n",
    "    print(f\"\\n[Method 3] Feature Selection + Linear Regression (k={k})\")\n",
    "    \n",
    "    X_train_top = X_train[top_k_cols]\n",
    "    X_test_top = X_test[top_k_cols]\n",
    "\n",
    "    for fs_name, fs_func in fs_methods.items():\n",
    "        print(f\"\\n  Evaluating: {fs_name} + LinearRegression\")\n",
    "        \n",
    "        best_r2 = -np.inf\n",
    "        best_n = None\n",
    "        best_metrics = {}\n",
    "        best_fold_results = []\n",
    "\n",
    "        # Try different numbers of features\n",
    "        for n in range(2, min(k + 1, len(top_k_cols) + 1)):\n",
    "            # Nested CV: outer loop for evaluation\n",
    "            fold_results = []\n",
    "            \n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X_train_top)):\n",
    "                X_fold_train = X_train_top.iloc[train_idx]\n",
    "                y_fold_train = y_train.iloc[train_idx]\n",
    "                X_fold_val = X_train_top.iloc[val_idx]\n",
    "                y_fold_val = y_train.iloc[val_idx]\n",
    "                \n",
    "                try:\n",
    "                    # Feature selection on training fold\n",
    "                    selected_feats = fs_func(X_fold_train, y_fold_train, n=n)\n",
    "                    X_fold_train_fs = X_fold_train[selected_feats]\n",
    "                    X_fold_val_fs = X_fold_val[selected_feats]\n",
    "                    \n",
    "                    # Train Linear Regression meta-model\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(X_fold_train_fs, y_fold_train)\n",
    "                    y_pred_val = model.predict(X_fold_val_fs)\n",
    "                    \n",
    "                    fold_results.append({\n",
    "                        'R2': r2_score(y_fold_val, y_pred_val),\n",
    "                        'MSE': mean_squared_error(y_fold_val, y_pred_val),\n",
    "                        'MAE': mean_absolute_error(y_fold_val, y_pred_val),\n",
    "                        'Selected_Features': list(selected_feats)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Failed for n={n}, fold={fold_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if len(fold_results) == CONFIG[\"cv_folds\"]:\n",
    "                avg_r2 = np.mean([f['R2'] for f in fold_results])\n",
    "                \n",
    "                if avg_r2 > best_r2:\n",
    "                    best_r2 = avg_r2\n",
    "                    best_n = n\n",
    "                    best_fold_results = fold_results\n",
    "                    best_metrics = {\n",
    "                        \"CV_R2\": avg_r2,\n",
    "                        \"CV_R2_Std\": np.std([f['R2'] for f in fold_results]),\n",
    "                        \"CV_MSE\": np.mean([f['MSE'] for f in fold_results]),\n",
    "                        \"CV_MSE_Std\": np.std([f['MSE'] for f in fold_results]),\n",
    "                        \"CV_MAE\": np.mean([f['MAE'] for f in fold_results]),\n",
    "                        \"CV_MAE_Std\": np.std([f['MAE'] for f in fold_results]),\n",
    "                    }\n",
    "        \n",
    "        if best_n is None:\n",
    "            print(f\"    ⚠ Skipped: No valid configuration found\")\n",
    "            continue\n",
    "        \n",
    "        # Store fold-wise results for best n\n",
    "        for fold_num, fold_result in enumerate(best_fold_results):\n",
    "            all_fold_metrics.append({\n",
    "                'Method': 'FS_MetaModel',\n",
    "                'Top_K': k,\n",
    "                'Meta_Model': 'LinearRegression',\n",
    "                'FS_Method': fs_name,\n",
    "                'Best_n': best_n,\n",
    "                'Fold': fold_num + 1,\n",
    "                'R2': fold_result['R2'],\n",
    "                'MSE': fold_result['MSE'],\n",
    "                'MAE': fold_result['MAE']\n",
    "            })\n",
    "        \n",
    "        # Final test evaluation with best n\n",
    "        try:\n",
    "            selected_feats = fs_func(X_train_top, y_train, n=best_n)\n",
    "            X_train_fs = X_train_top[selected_feats]\n",
    "            X_test_fs = X_test_top[selected_feats]\n",
    "            \n",
    "            final_model = LinearRegression()\n",
    "            final_model.fit(X_train_fs, y_train)\n",
    "            y_pred_test = final_model.predict(X_test_fs)\n",
    "            \n",
    "            test_r2 = r2_score(y_test, y_pred_test)\n",
    "            test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "            test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "            \n",
    "            print(f\"    CV R² = {best_metrics['CV_R2']:.4f} (±{best_metrics['CV_R2_Std']:.4f}), Test R² = {test_r2:.4f} (n={best_n})\")\n",
    "            \n",
    "            best_results.append({\n",
    "                \"Dataset\": dataset_slug,\n",
    "                \"Method\": \"FS_MetaModel\",\n",
    "                \"Top_K\": k,\n",
    "                \"Meta_Model\": \"LinearRegression\",\n",
    "                \"FS_Method\": fs_name,\n",
    "                \"Best_n\": best_n,\n",
    "                **best_metrics,\n",
    "                \"Test_R2\": test_r2,\n",
    "                \"Test_MSE\": test_mse,\n",
    "                \"Test_MAE\": test_mae,\n",
    "                \"Selected_Features\": ','.join(list(selected_feats)[:5]) + '...' if len(selected_feats) > 5 else ','.join(list(selected_feats))\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"     Failed final evaluation: {e}\")\n",
    "\n",
    "# ---------------------\n",
    "# Save Results\n",
    "# ---------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save aggregated results\n",
    "results_df = pd.DataFrame(best_results)\n",
    "results_df = results_df.sort_values('Test_R2', ascending=False)\n",
    "results_path = os.path.join(output_dir, \"best_meta_model_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\" Saved aggregated results: {results_path}\")\n",
    "\n",
    "# Save fold-wise metrics\n",
    "fold_metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "fold_metrics_path = os.path.join(fold_metrics_dir, \"stage2_fold_metrics.csv\")\n",
    "fold_metrics_df.to_csv(fold_metrics_path, index=False)\n",
    "print(f\" Saved fold-wise metrics: {fold_metrics_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal configurations evaluated: {len(results_df)}\")\n",
    "print(f\"\\nTop 5 Configurations by Test R²:\")\n",
    "print(results_df[['Method', 'Top_K', 'Meta_Model', 'FS_Method', 'Best_n', 'CV_R2', 'CV_R2_Std', 'Test_R2']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2 COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6020f2a-38a6-4a7b-b07d-e53494b88e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
