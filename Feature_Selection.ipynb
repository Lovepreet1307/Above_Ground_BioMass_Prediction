{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad3ebc6-297d-48fc-a020-071d9f1d7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RF with RF feature selection...\n",
      "  → HPO for RF with RF using top-5 features\n",
      "  → HPO for RF with RF using top-10 features\n",
      "  → HPO for RF with RF using top-15 features\n",
      "  → HPO for RF with RF using top-16 features\n",
      "\n",
      "Evaluating XGB with RF feature selection...\n",
      "  → HPO for XGB with RF using top-5 features\n",
      "  → HPO for XGB with RF using top-10 features\n",
      "  → HPO for XGB with RF using top-15 features\n",
      "  → HPO for XGB with RF using top-20 features\n",
      "  → HPO for XGB with RF using top-25 features\n",
      "  → HPO for XGB with RF using top-30 features\n",
      "  → HPO for XGB with RF using top-35 features\n",
      "  → HPO for XGB with RF using top-36 features\n",
      "\n",
      "Evaluating RF with RFE_RF feature selection...\n",
      "  → HPO for RF with RFE_RF using top-5 features\n",
      "  → HPO for RF with RFE_RF using top-10 features\n",
      "  → HPO for RF with RFE_RF using top-15 features\n",
      "  → HPO for RF with RFE_RF using top-20 features\n",
      "\n",
      "Evaluating XGB with RFE_RF feature selection...\n",
      "  → HPO for XGB with RFE_RF using top-5 features\n",
      "  → HPO for XGB with RFE_RF using top-10 features\n",
      "  → HPO for XGB with RFE_RF using top-15 features\n",
      "  → HPO for XGB with RFE_RF using top-20 features\n",
      "  → HPO for XGB with RFE_RF using top-25 features\n",
      "  → HPO for XGB with RFE_RF using top-30 features\n",
      "  → HPO for XGB with RFE_RF using top-35 features\n",
      "\n",
      "Evaluating RF with XGB feature selection...\n",
      "  → HPO for RF with XGB using top-5 features\n",
      "  → HPO for RF with XGB using top-10 features\n",
      "  → HPO for RF with XGB using top-14 features\n",
      "\n",
      "Evaluating XGB with XGB feature selection...\n",
      "  → HPO for XGB with XGB using top-5 features\n",
      "  → HPO for XGB with XGB using top-10 features\n",
      "  → HPO for XGB with XGB using top-15 features\n",
      "  → HPO for XGB with XGB using top-20 features\n",
      "\n",
      "Evaluating RF with RF_XGB feature selection...\n",
      "  → HPO for RF with RF_XGB using top-5 features\n",
      "  → HPO for RF with RF_XGB using top-10 features\n",
      "  → HPO for RF with RF_XGB using top-15 features\n",
      "\n",
      "Evaluating XGB with RF_XGB feature selection...\n",
      "  → HPO for XGB with RF_XGB using top-5 features\n",
      "  → HPO for XGB with RF_XGB using top-10 features\n",
      "  → HPO for XGB with RF_XGB using top-15 features\n",
      "  → HPO for XGB with RF_XGB using top-17 features\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(CONFIG[\"predictions_dir\"], exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "os.makedirs(\"hyperparams\", exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y, \n",
    "                            test_size=CONFIG[\"test_size\"],\n",
    "                            random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    rankings = {}\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                                random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_scores = rf.feature_importances_\n",
    "    rankings['RF'] = list(X_train.columns[np.argsort(rf_scores)[::-1]])\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "              n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_scores = rfe.ranking_\n",
    "    rankings['RFE_RF'] = list(X_train.columns[np.argsort(rfe_scores)])\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "                       random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_scores = xgb.feature_importances_\n",
    "    rankings['XGB'] = list(X_train.columns[np.argsort(xgb_scores)[::-1]])\n",
    "\n",
    "    hybrid_scores = (rf_scores + xgb_scores) / 2\n",
    "    rankings['RF_XGB'] = list(X_train.columns[np.argsort(hybrid_scores)[::-1]])\n",
    "\n",
    "    return rankings\n",
    "\n",
    "def get_model_config(model_name):\n",
    "    if model_name == 'RF':\n",
    "        model_class = RandomForestRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    elif model_name == 'XGB':\n",
    "        model_class = XGBRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    return model_class, param_grid\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    scoring = {\n",
    "    'r2': 'r2',\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'mse': 'neg_mean_squared_error'\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring, #CONFIG[\"metric\"],  \n",
    "        cv=CONFIG[\"cv_folds\"],\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        refit=\"r2\"\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    ind=np.argmax(grid_search.cv_results_[\"mean_test_r2\"])\n",
    "    r2 = grid_search.cv_results_[\"mean_test_r2\"][ind]\n",
    "    mae = -1*grid_search.cv_results_['mean_test_mae'][ind]\n",
    "    mse = -1*grid_search.cv_results_[\"mean_test_mse\"][ind]\n",
    "    return {\n",
    "        \"best_model\": grid_search.best_estimator_,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"r2\": r2,\n",
    "        \"mse\":mse,\n",
    "        \"mae\":mae\n",
    "    }\n",
    "\n",
    "def run_pipeline():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    all_results = []\n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    cv_train_preds = pd.DataFrame({'true': y_train.reset_index(drop=True)})\n",
    "\n",
    "    for fs_name, features in feature_rankings.items():\n",
    "        for model_name, Model in [('RF', RandomForestRegressor), ('XGB', XGBRegressor)]:\n",
    "            print(f\"\\nEvaluating {model_name} with {fs_name} feature selection...\")\n",
    "\n",
    "            best_k = None\n",
    "            best_cv_r2 = -np.inf\n",
    "\n",
    "            # Step 1: Find best k using CV\n",
    "            for k in range(1, len(features) + 1):\n",
    "                k_features = features[:k]\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                avg_r2 = np.mean(fold_r2)\n",
    "\n",
    "                if avg_r2 > best_cv_r2:\n",
    "                    best_cv_r2 = avg_r2\n",
    "                    best_k = k\n",
    "\n",
    "            # Step 2: HPO for spaced subset sizes [5, 10, ..., best_k]\n",
    "            subset_ks = [k for k in range(5, best_k + 1, 5)]\n",
    "            if best_k not in subset_ks:\n",
    "                subset_ks.append(best_k)\n",
    "\n",
    "            for k in subset_ks:\n",
    "                k_features = features[:k]\n",
    "                fold_preds = np.zeros(len(y_train))\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_preds[val_idx] = preds\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                cv_r2 = np.mean(fold_r2)\n",
    "                cv_mse = mean_squared_error(y_train, fold_preds)\n",
    "                cv_mae = mean_absolute_error(y_train, fold_preds)\n",
    "\n",
    "                model_class, param_grid = get_model_config(model_name)\n",
    "                print(f\"  → HPO for {model_name} with {fs_name} using top-{k} features\")\n",
    "                hpo_results = tune_hyperparameters(\n",
    "                    model_class(random_state=CONFIG[\"random_state\"]),\n",
    "                    param_grid,\n",
    "                    X_train[k_features],\n",
    "                    y_train\n",
    "                )\n",
    "\n",
    "                test_preds = hpo_results[\"best_model\"].predict(X_test[k_features])\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                test_mse = mean_squared_error(y_test, test_preds)\n",
    "                test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "                model_id = f\"{fs_name}_{model_name}_Top{k}\"\n",
    "                combined_preds[f'pred_{model_id}'] = test_preds\n",
    "                cv_train_preds[f'pred_{model_id}'] = fold_preds\n",
    "\n",
    "                joblib.dump(hpo_results[\"best_model\"], f\"models/Model_{model_id}.pkl\")\n",
    "\n",
    "                with open(f\"features/Features_{model_id}.json\", 'w') as f:\n",
    "                    json.dump(k_features, f)\n",
    "\n",
    "                with open(f\"hyperparams/BestParams_{model_id}.json\", 'w') as f:\n",
    "                    json.dump(hpo_results[\"best_params\"], f)\n",
    "\n",
    "                all_results.append({\n",
    "                    'FS_Method': fs_name,\n",
    "                    'Model': model_name,\n",
    "                    'Features_Used': k,\n",
    "                    'Best_Params': hpo_results[\"best_params\"],\n",
    "                    'CV_R2': cv_r2,\n",
    "                    'CV_MSE': cv_mse,\n",
    "                    'CV_MAE': cv_mae,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Test_MSE': test_mse,\n",
    "                    'Test_MAE': test_mae\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_results)\n",
    "    metrics_df['R2_Rank'] = metrics_df['Test_R2'].rank(ascending=False, method='min')\n",
    "    metrics_df = metrics_df.sort_values('R2_Rank')\n",
    "    metrics_df.to_csv(f\"metrics/{CONFIG['results_csv_path']}\", index=False)\n",
    "    combined_preds.to_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\", index=False)\n",
    "    cv_train_preds.to_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f66196d-412c-4ec4-a4c2-8dcd128838db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 1 & 2: Ensemble Evaluation for k in [10, 20, ..., Max] ===\n",
      "\n",
      "--- Method 1: Random 10 Model Averaging ---\n",
      "Train CV R2 : 0.7713080189463937\n",
      "Test     R2 : 0.7756311450908048\n",
      "Test     MSE: 4743.854971616518\n",
      "Test     MAE: 48.87691787320144\n",
      "\n",
      "--- Method 2: Top 10 Models (CV_R2) — Simple Average ---\n",
      "Test     R2 : 0.7737461576423652\n",
      "Test     MSE: 4783.709465157226\n",
      "Test     MAE: 50.58740458016972\n",
      "\n",
      "--- Method 1: Random 20 Model Averaging ---\n",
      "Train CV R2 : 0.7760999320060629\n",
      "Test     R2 : 0.7724348260457022\n",
      "Test     MSE: 4811.435090965608\n",
      "Test     MAE: 49.34603622716597\n",
      "\n",
      "--- Method 2: Top 20 Models (CV_R2) — Simple Average ---\n",
      "Test     R2 : 0.7742757121238147\n",
      "Test     MSE: 4772.51303746862\n",
      "Test     MAE: 50.236808263367756\n",
      "\n",
      "--- Method 1: Random 30 Model Averaging ---\n",
      "Train CV R2 : 0.7751716608227526\n",
      "Test     R2 : 0.7754275894551974\n",
      "Test     MSE: 4748.158770440857\n",
      "Test     MAE: 49.03559251490448\n",
      "\n",
      "--- Method 2: Top 30 Models (CV_R2) — Simple Average ---\n",
      "Test     R2 : 0.7782642945333733\n",
      "Test     MSE: 4688.182008097612\n",
      "Test     MAE: 49.415402754458256\n",
      "\n",
      "--- Method 1: Random 37 Model Averaging ---\n",
      "Train CV R2 : 0.7756957953805341\n",
      "Test     R2 : 0.7756553430950166\n",
      "Test     MSE: 4743.343350595723\n",
      "Test     MAE: 49.00236150419841\n",
      "\n",
      "--- Method 2: Top 37 Models (CV_R2) — Simple Average ---\n",
      "Test     R2 : 0.7756553430950165\n",
      "Test     MSE: 4743.343350595724\n",
      "Test     MAE: 49.00236150419842\n",
      "\n",
      "=== Method 3: FS + Meta Models for Top-k Models ===\n",
      "\n",
      "--- Evaluating Method 3 with Top-10 Models ---\n",
      "Top-10 → RF + LinearRegression: Best R2 = 0.7776 (n=2)\n",
      "Top-10 → RF + RandomForest: Best R2 = 0.7499 (n=2)\n",
      "Top-10 → RF + XGBoost: Best R2 = 0.7334 (n=10)\n",
      "Top-10 → RFE_RF + LinearRegression: Best R2 = 0.7727 (n=5)\n",
      "Top-10 → RFE_RF + RandomForest: Best R2 = 0.7421 (n=9)\n",
      "Top-10 → RFE_RF + XGBoost: Best R2 = 0.7240 (n=10)\n",
      "Top-10 → XGB + LinearRegression: Best R2 = 0.7758 (n=4)\n",
      "Top-10 → XGB + RandomForest: Best R2 = 0.7409 (n=9)\n",
      "Top-10 → XGB + XGBoost: Best R2 = 0.7422 (n=10)\n",
      "Top-10 → RF_XGB + LinearRegression: Best R2 = 0.7770 (n=3)\n",
      "Top-10 → RF_XGB + RandomForest: Best R2 = 0.7408 (n=9)\n",
      "Top-10 → RF_XGB + XGBoost: Best R2 = 0.7459 (n=10)\n",
      "\n",
      "--- Evaluating Method 3 with Top-20 Models ---\n",
      "Top-20 → RF + LinearRegression: Best R2 = 0.7737 (n=6)\n",
      "Top-20 → RF + RandomForest: Best R2 = 0.7635 (n=17)\n",
      "Top-20 → RF + XGBoost: Best R2 = 0.7468 (n=19)\n",
      "Top-20 → RFE_RF + LinearRegression: Best R2 = 0.7750 (n=2)\n",
      "Top-20 → RFE_RF + RandomForest: Best R2 = 0.7632 (n=18)\n",
      "Top-20 → RFE_RF + XGBoost: Best R2 = 0.7580 (n=20)\n",
      "Top-20 → XGB + LinearRegression: Best R2 = 0.7737 (n=6)\n",
      "Top-20 → XGB + RandomForest: Best R2 = 0.7757 (n=3)\n",
      "Top-20 → XGB + XGBoost: Best R2 = 0.7554 (n=20)\n",
      "Top-20 → RF_XGB + LinearRegression: Best R2 = 0.7750 (n=2)\n",
      "Top-20 → RF_XGB + RandomForest: Best R2 = 0.7654 (n=19)\n",
      "Top-20 → RF_XGB + XGBoost: Best R2 = 0.7633 (n=19)\n",
      "\n",
      "--- Evaluating Method 3 with Top-30 Models ---\n",
      "Top-30 → RF + LinearRegression: Best R2 = 0.7750 (n=2)\n",
      "Top-30 → RF + RandomForest: Best R2 = 0.7600 (n=23)\n",
      "Top-30 → RF + XGBoost: Best R2 = 0.7397 (n=27)\n",
      "Top-30 → RFE_RF + LinearRegression: Best R2 = 0.7750 (n=2)\n",
      "Top-30 → RFE_RF + RandomForest: Best R2 = 0.7576 (n=23)\n",
      "Top-30 → RFE_RF + XGBoost: Best R2 = 0.7465 (n=20)\n",
      "Top-30 → XGB + LinearRegression: Best R2 = 0.7769 (n=4)\n",
      "Top-30 → XGB + RandomForest: Best R2 = 0.7766 (n=9)\n",
      "Top-30 → XGB + XGBoost: Best R2 = 0.7664 (n=6)\n",
      "Top-30 → RF_XGB + LinearRegression: Best R2 = 0.7771 (n=3)\n",
      "Top-30 → RF_XGB + RandomForest: Best R2 = 0.7508 (n=22)\n",
      "Top-30 → RF_XGB + XGBoost: Best R2 = 0.7507 (n=29)\n",
      "\n",
      "--- Evaluating Method 3 with Top-37 Models ---\n",
      "Top-37 → RF + LinearRegression: Best R2 = 0.7750 (n=2)\n",
      "Top-37 → RF + RandomForest: Best R2 = 0.7822 (n=2)\n",
      "Top-37 → RF + XGBoost: Best R2 = 0.7544 (n=21)\n",
      "Top-37 → RFE_RF + LinearRegression: Best R2 = 0.7750 (n=3)\n",
      "Top-37 → RFE_RF + RandomForest: Best R2 = 0.7887 (n=3)\n",
      "Top-37 → RFE_RF + XGBoost: Best R2 = 0.7433 (n=24)\n",
      "Top-37 → XGB + LinearRegression: Best R2 = 0.7756 (n=7)\n",
      "Top-37 → XGB + RandomForest: Best R2 = 0.7539 (n=32)\n",
      "Top-37 → XGB + XGBoost: Best R2 = 0.7302 (n=3)\n",
      "Top-37 → RF_XGB + LinearRegression: Best R2 = 0.7750 (n=3)\n",
      "Top-37 → RF_XGB + RandomForest: Best R2 = 0.7827 (n=3)\n",
      "Top-37 → RF_XGB + XGBoost: Best R2 = 0.7250 (n=26)\n",
      "\n",
      "Saved Method 3 results to: metrics/best_meta_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ---------------------\n",
    "# Load config & data\n",
    "# ---------------------\n",
    "with open(\"config.json\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "cv_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\")\n",
    "test_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\")\n",
    "metrics_df = pd.read_csv(f\"metrics/{CONFIG['results_csv_path']}\")\n",
    "\n",
    "X_train = cv_df.drop(columns=[\"true\"])\n",
    "y_train = cv_df[\"true\"]\n",
    "X_test = test_df.drop(columns=[\"true\"])\n",
    "y_test = test_df[\"true\"]\n",
    "\n",
    "model_columns = X_train.columns.tolist()\n",
    "\n",
    "# ---------------------\n",
    "# Feature Selection Methods\n",
    "# ---------------------\n",
    "def fs_rf(X, y, n):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    scores = rf.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rfe_rf(X, y, n):\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=n)\n",
    "    rfe.fit(X, y)\n",
    "    return X.columns[rfe.support_]\n",
    "\n",
    "def fs_xgb(X, y, n):\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    xgb.fit(X, y)\n",
    "    scores = xgb.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rf_xgb(X, y, n):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    xgb.fit(X, y)\n",
    "    avg_scores = (rf.feature_importances_ + xgb.feature_importances_) / 2\n",
    "    return X.columns[np.argsort(avg_scores)[::-1][:n]]\n",
    "\n",
    "fs_methods = {\n",
    "    \"RF\": fs_rf,\n",
    "    \"RFE_RF\": fs_rfe_rf,\n",
    "    \"XGB\": fs_xgb,\n",
    "    \"RF_XGB\": fs_rf_xgb\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Meta-models\n",
    "# ---------------------------------------\n",
    "meta_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "    \"XGBoost\": XGBRegressor(random_state=CONFIG[\"random_state\"])\n",
    "}\n",
    "\n",
    "# ---------------------------------------\n",
    "# Shared Top-K Range\n",
    "# ---------------------------------------\n",
    "max_models = len(metrics_df)\n",
    "step = 10\n",
    "k_values = list(range(10, max_models, step))\n",
    "if max_models not in k_values:\n",
    "    k_values.append(max_models)\n",
    "\n",
    "# ---------------------\n",
    "# Method 1 & 2: Varying Top-k Model Ensemble\n",
    "# ---------------------\n",
    "print(\"\\n=== Method 1 & 2: Ensemble Evaluation for k in [10, 20, ..., Max] ===\")\n",
    "\n",
    "for k in k_values:\n",
    "    # Method 1: Random k models\n",
    "    random_cols = random.sample(model_columns, k)\n",
    "    y_pred_train_random = X_train[random_cols].mean(axis=1)\n",
    "    y_pred_test_random = X_test[random_cols].mean(axis=1)\n",
    "\n",
    "    print(f\"\\n--- Method 1: Random {k} Model Averaging ---\")\n",
    "    print(\"Train CV R2 :\", r2_score(y_train, y_pred_train_random))\n",
    "    print(\"Test     R2 :\", r2_score(y_test, y_pred_test_random))\n",
    "    print(\"Test     MSE:\", mean_squared_error(y_test, y_pred_test_random))\n",
    "    print(\"Test     MAE:\", mean_absolute_error(y_test, y_pred_test_random))\n",
    "\n",
    "    # Method 2: Top k models by CV_R2\n",
    "    top_k_models = metrics_df.sort_values(\"CV_R2\", ascending=False).head(k)\n",
    "    top_k_cols = [\n",
    "        f\"pred_{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "        for _, row in top_k_models.iterrows()\n",
    "    ]\n",
    "\n",
    "    y_pred_test_top = X_test[top_k_cols].mean(axis=1)\n",
    "\n",
    "    print(f\"\\n--- Method 2: Top {k} Models (CV_R2) — Simple Average ---\")\n",
    "    print(\"Test     R2 :\", r2_score(y_test, y_pred_test_top))\n",
    "    print(\"Test     MSE:\", mean_squared_error(y_test, y_pred_test_top))\n",
    "    print(\"Test     MAE:\", mean_absolute_error(y_test, y_pred_test_top))\n",
    "\n",
    "# ---------------------\n",
    "# Method 3: Feature Selection + Meta Model (on model predictions)\n",
    "# ---------------------\n",
    "print(\"\\n=== Method 3: FS + Meta Models for Top-k Models ===\")\n",
    "\n",
    "all_method3_results = []\n",
    "\n",
    "for top_k in k_values:\n",
    "    print(f\"\\n--- Evaluating Method 3 with Top-{top_k} Models ---\")\n",
    "\n",
    "    top_k_models = metrics_df.sort_values(\"CV_R2\", ascending=False).head(top_k)\n",
    "    top_model_ids = [\n",
    "        f\"pred_{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "        for _, row in top_k_models.iterrows()\n",
    "    ]\n",
    "\n",
    "    X_train_top = X_train[top_model_ids]\n",
    "    X_test_top = X_test[top_model_ids]\n",
    "\n",
    "    for fs_name, fs_func in fs_methods.items():\n",
    "        for model_name, model in meta_models.items():\n",
    "            best_r2 = -np.inf\n",
    "            best_n = None\n",
    "            best_metrics = {}\n",
    "\n",
    "            for n in range(2, top_k + 1):\n",
    "                if n > len(top_model_ids):\n",
    "                    break\n",
    "                selected_feats = fs_func(X_train_top, y_train, n=n)\n",
    "                X_train_fs = X_train_top[selected_feats]\n",
    "                X_test_fs = X_test_top[selected_feats]\n",
    "\n",
    "                model.fit(X_train_fs, y_train)\n",
    "                y_pred = model.predict(X_test_fs)\n",
    "\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_n = n\n",
    "                    best_metrics = {\n",
    "                        \"R2\": r2,\n",
    "                        \"MSE\": mse,\n",
    "                        \"MAE\": mae,\n",
    "                        \"Selected_Features\": list(selected_feats)\n",
    "                    }\n",
    "\n",
    "            print(f\"Top-{top_k} → {fs_name} + {model_name}: Best R2 = {best_r2:.4f} (n={best_n})\")\n",
    "\n",
    "            all_method3_results.append({\n",
    "                \"Method\": \"FS_MetaModel\",\n",
    "                \"Top_K\": top_k,\n",
    "                \"Meta_Model\": model_name,\n",
    "                \"FS_Method\": fs_name,\n",
    "                \"Best_n\": best_n,\n",
    "                **best_metrics\n",
    "            })\n",
    "\n",
    "# Save Method 3 results\n",
    "pd.DataFrame(all_method3_results).to_csv(\"metrics/best_meta_model_results.csv\", index=False)\n",
    "print(\"\\nSaved Method 3 results to: metrics/best_meta_model_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17fc54-0263-45ce-9704-8570618b2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RF with RF feature selection...\n",
      "  → HPO for RF with RF using top-1 features\n",
      "  → HPO for RF with RF using top-2 features\n",
      "  → HPO for RF with RF using top-3 features\n",
      "  → HPO for RF with RF using top-4 features\n",
      "  → HPO for RF with RF using top-5 features\n",
      "  → HPO for RF with RF using top-6 features\n",
      "\n",
      "Evaluating XGB with RF feature selection...\n",
      "  → HPO for XGB with RF using top-1 features\n",
      "  → HPO for XGB with RF using top-2 features\n",
      "  → HPO for XGB with RF using top-3 features\n",
      "  → HPO for XGB with RF using top-4 features\n",
      "  → HPO for XGB with RF using top-5 features\n",
      "  → HPO for XGB with RF using top-6 features\n",
      "  → HPO for XGB with RF using top-7 features\n",
      "  → HPO for XGB with RF using top-8 features\n",
      "\n",
      "Evaluating RF with RFE_RF feature selection...\n",
      "  → HPO for RF with RFE_RF using top-1 features\n",
      "  → HPO for RF with RFE_RF using top-2 features\n",
      "  → HPO for RF with RFE_RF using top-3 features\n",
      "  → HPO for RF with RFE_RF using top-4 features\n",
      "  → HPO for RF with RFE_RF using top-5 features\n",
      "  → HPO for RF with RFE_RF using top-6 features\n",
      "\n",
      "Evaluating XGB with RFE_RF feature selection...\n",
      "  → HPO for XGB with RFE_RF using top-1 features\n",
      "  → HPO for XGB with RFE_RF using top-2 features\n",
      "  → HPO for XGB with RFE_RF using top-3 features\n",
      "  → HPO for XGB with RFE_RF using top-4 features\n",
      "  → HPO for XGB with RFE_RF using top-5 features\n",
      "  → HPO for XGB with RFE_RF using top-6 features\n",
      "  → HPO for XGB with RFE_RF using top-7 features\n",
      "  → HPO for XGB with RFE_RF using top-8 features\n",
      "\n",
      "Evaluating RF with XGB feature selection...\n",
      "  → HPO for RF with XGB using top-1 features\n",
      "  → HPO for RF with XGB using top-2 features\n",
      "  → HPO for RF with XGB using top-3 features\n",
      "  → HPO for RF with XGB using top-4 features\n",
      "  → HPO for RF with XGB using top-5 features\n",
      "  → HPO for RF with XGB using top-6 features\n",
      "  → HPO for RF with XGB using top-7 features\n",
      "\n",
      "Evaluating XGB with XGB feature selection...\n",
      "  → HPO for XGB with XGB using top-1 features\n",
      "  → HPO for XGB with XGB using top-2 features\n",
      "  → HPO for XGB with XGB using top-3 features\n",
      "  → HPO for XGB with XGB using top-4 features\n",
      "  → HPO for XGB with XGB using top-5 features\n",
      "  → HPO for XGB with XGB using top-6 features\n",
      "\n",
      "Evaluating RF with RF_XGB feature selection...\n",
      "  → HPO for RF with RF_XGB using top-1 features\n",
      "  → HPO for RF with RF_XGB using top-2 features\n",
      "  → HPO for RF with RF_XGB using top-3 features\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create dataset-specific folder name\n",
    "dataset_slug = re.sub(r'\\W+', '_', os.path.splitext(os.path.basename(CONFIG[\"dataset_path\"]))[0])\n",
    "\n",
    "# Define dataset-specific output directories\n",
    "base_dirs = {\n",
    "    \"predictions\": os.path.join(CONFIG[\"predictions_dir\"], dataset_slug),\n",
    "    \"models\": os.path.join(\"models\", dataset_slug),\n",
    "    \"features\": os.path.join(\"features\", dataset_slug),\n",
    "    \"metrics\": os.path.join(\"metrics\", dataset_slug),\n",
    "    \"hyperparams\": os.path.join(\"hyperparams\", dataset_slug)\n",
    "}\n",
    "\n",
    "# Create all output directories\n",
    "for path in base_dirs.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y,\n",
    "                            test_size=CONFIG[\"test_size\"],\n",
    "                            random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    rankings = {}\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                                random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_scores = rf.feature_importances_\n",
    "    rankings['RF'] = list(X_train.columns[np.argsort(rf_scores)[::-1]])\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "              n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_scores = rfe.ranking_\n",
    "    rankings['RFE_RF'] = list(X_train.columns[np.argsort(rfe_scores)])\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "                       random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_scores = xgb.feature_importances_\n",
    "    rankings['XGB'] = list(X_train.columns[np.argsort(xgb_scores)[::-1]])\n",
    "\n",
    "    hybrid_scores = (rf_scores + xgb_scores) / 2\n",
    "    rankings['RF_XGB'] = list(X_train.columns[np.argsort(hybrid_scores)[::-1]])\n",
    "\n",
    "    return rankings\n",
    "\n",
    "def get_model_config(model_name):\n",
    "    if model_name == 'RF':\n",
    "        model_class = RandomForestRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    elif model_name == 'XGB':\n",
    "        model_class = XGBRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    return model_class, param_grid\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    scoring = {\n",
    "        'r2': 'r2',\n",
    "        'mae': 'neg_mean_absolute_error',\n",
    "        'mse': 'neg_mean_squared_error'\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        cv=CONFIG[\"cv_folds\"],\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        refit=\"r2\"\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    ind = np.argmax(grid_search.cv_results_[\"mean_test_r2\"])\n",
    "    r2 = grid_search.cv_results_[\"mean_test_r2\"][ind]\n",
    "    mae = -1 * grid_search.cv_results_['mean_test_mae'][ind]\n",
    "    mse = -1 * grid_search.cv_results_[\"mean_test_mse\"][ind]\n",
    "    return {\n",
    "        \"best_model\": grid_search.best_estimator_,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"r2\": r2,\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae\n",
    "    }\n",
    "\n",
    "def run_pipeline():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    all_results = []\n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    cv_train_preds = pd.DataFrame({'true': y_train.reset_index(drop=True)})\n",
    "\n",
    "    for fs_name, features in feature_rankings.items():\n",
    "        for model_name, Model in [('RF', RandomForestRegressor), ('XGB', XGBRegressor)]:\n",
    "            print(f\"\\nEvaluating {model_name} with {fs_name} feature selection...\")\n",
    "\n",
    "            best_k = None\n",
    "            best_cv_r2 = -np.inf\n",
    "\n",
    "            # Step 1: Find best k using CV\n",
    "            for k in range(1, len(features) + 1):\n",
    "                k_features = features[:k]\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                avg_r2 = np.mean(fold_r2)\n",
    "\n",
    "                if avg_r2 > best_cv_r2:\n",
    "                    best_cv_r2 = avg_r2\n",
    "                    best_k = k\n",
    "\n",
    "            # Step 2: HPO for spaced subset sizes\n",
    "            subset_k_start = CONFIG.get(\"subset_k_start\")\n",
    "            subset_k_step = CONFIG.get(\"subset_k_step\")\n",
    "            subset_ks = [k for k in range(subset_k_start, best_k + 1, subset_k_step)]\n",
    "            if best_k not in subset_ks:\n",
    "                subset_ks.append(best_k)\n",
    "\n",
    "            for k in subset_ks:\n",
    "                k_features = features[:k]\n",
    "                fold_preds = np.zeros(len(y_train))\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_preds[val_idx] = preds\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                cv_r2 = np.mean(fold_r2)\n",
    "                cv_mse = mean_squared_error(y_train, fold_preds)\n",
    "                cv_mae = mean_absolute_error(y_train, fold_preds)\n",
    "\n",
    "                model_class, param_grid = get_model_config(model_name)\n",
    "                print(f\"  → HPO for {model_name} with {fs_name} using top-{k} features\")\n",
    "                hpo_results = tune_hyperparameters(\n",
    "                    model_class(random_state=CONFIG[\"random_state\"]),\n",
    "                    param_grid,\n",
    "                    X_train[k_features],\n",
    "                    y_train\n",
    "                )\n",
    "\n",
    "                test_preds = hpo_results[\"best_model\"].predict(X_test[k_features])\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                test_mse = mean_squared_error(y_test, test_preds)\n",
    "                test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "                model_id = f\"{fs_name}_{model_name}_Top{k}\"\n",
    "                combined_preds[f'pred_{model_id}'] = test_preds\n",
    "                cv_train_preds[f'pred_{model_id}'] = fold_preds\n",
    "\n",
    "                joblib.dump(hpo_results[\"best_model\"], os.path.join(base_dirs[\"models\"], f\"Model_{model_id}.pkl\"))\n",
    "\n",
    "                with open(os.path.join(base_dirs[\"features\"], f\"Features_{model_id}.json\"), 'w') as f:\n",
    "                    json.dump(k_features, f)\n",
    "\n",
    "                with open(os.path.join(base_dirs[\"hyperparams\"], f\"BestParams_{model_id}.json\"), 'w') as f:\n",
    "                    json.dump(hpo_results[\"best_params\"], f)\n",
    "\n",
    "                all_results.append({\n",
    "                    'FS_Method': fs_name,\n",
    "                    'Model': model_name,\n",
    "                    'Features_Used': k,\n",
    "                    'Best_Params': hpo_results[\"best_params\"],\n",
    "                    'CV_R2': cv_r2,\n",
    "                    'CV_MSE': cv_mse,\n",
    "                    'CV_MAE': cv_mae,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Test_MSE': test_mse,\n",
    "                    'Test_MAE': test_mae\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_results)\n",
    "    metrics_df['R2_Rank'] = metrics_df['Test_R2'].rank(ascending=False, method='min')\n",
    "    metrics_df = metrics_df.sort_values('R2_Rank')\n",
    "    metrics_df.to_csv(os.path.join(base_dirs[\"metrics\"], CONFIG['results_csv_path']), index=False)\n",
    "    combined_preds.to_csv(os.path.join(base_dirs[\"predictions\"], \"all_predictions.csv\"), index=False)\n",
    "    cv_train_preds.to_csv(os.path.join(base_dirs[\"predictions\"], \"cv_train_predictions.csv\"), index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc37f10-0e13-4cb6-aff1-13ba4d62e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Methods 1, 2, and 3 across Top-K values ===\n",
      "\n",
      "========== Top-10 Models ==========\n",
      "Method 1 - Random 10: R2 = 0.8529\n",
      "Method 2 - Top 10: R2 = 0.8552\n",
      "Method 3 - Top-10 → RF + LinearRegression: Best R2 = 0.8515 (n=9)\n",
      "Method 3 - Top-10 → RF + RandomForest: Best R2 = 0.7411 (n=6)\n",
      "Method 3 - Top-10 → RF + XGBoost: Best R2 = 0.7585 (n=10)\n",
      "Method 3 - Top-10 → RFE_RF + LinearRegression: Best R2 = 0.8515 (n=9)\n",
      "Method 3 - Top-10 → RFE_RF + RandomForest: Best R2 = 0.7386 (n=7)\n",
      "Method 3 - Top-10 → RFE_RF + XGBoost: Best R2 = 0.7566 (n=9)\n",
      "Method 3 - Top-10 → XGB + LinearRegression: Best R2 = 0.8592 (n=8)\n",
      "Method 3 - Top-10 → XGB + RandomForest: Best R2 = 0.7995 (n=2)\n",
      "Method 3 - Top-10 → XGB + XGBoost: Best R2 = 0.7864 (n=2)\n",
      "Method 3 - Top-10 → RF_XGB + LinearRegression: Best R2 = 0.8535 (n=2)\n",
      "Method 3 - Top-10 → RF_XGB + RandomForest: Best R2 = 0.7995 (n=2)\n",
      "Method 3 - Top-10 → RF_XGB + XGBoost: Best R2 = 0.7864 (n=2)\n",
      "\n",
      "========== Top-20 Models ==========\n",
      "Method 1 - Random 20: R2 = 0.8547\n",
      "Method 2 - Top 20: R2 = 0.8522\n",
      "Method 3 - Top-20 → RF + LinearRegression: Best R2 = 0.8475 (n=20)\n",
      "Method 3 - Top-20 → RF + RandomForest: Best R2 = 0.8242 (n=3)\n",
      "Method 3 - Top-20 → RF + XGBoost: Best R2 = 0.8317 (n=15)\n",
      "Method 3 - Top-20 → RFE_RF + LinearRegression: Best R2 = 0.8475 (n=20)\n",
      "Method 3 - Top-20 → RFE_RF + RandomForest: Best R2 = 0.8151 (n=18)\n",
      "Method 3 - Top-20 → RFE_RF + XGBoost: Best R2 = 0.8296 (n=16)\n",
      "Method 3 - Top-20 → XGB + LinearRegression: Best R2 = 0.8543 (n=3)\n",
      "Method 3 - Top-20 → XGB + RandomForest: Best R2 = 0.8189 (n=20)\n",
      "Method 3 - Top-20 → XGB + XGBoost: Best R2 = 0.8202 (n=14)\n",
      "Method 3 - Top-20 → RF_XGB + LinearRegression: Best R2 = 0.8543 (n=3)\n",
      "Method 3 - Top-20 → RF_XGB + RandomForest: Best R2 = 0.8103 (n=12)\n",
      "Method 3 - Top-20 → RF_XGB + XGBoost: Best R2 = 0.8228 (n=12)\n",
      "\n",
      "========== Top-22 Models ==========\n",
      "Method 1 - Random 22: R2 = 0.8546\n",
      "Method 2 - Top 22: R2 = 0.8546\n",
      "Method 3 - Top-22 → RF + LinearRegression: Best R2 = 0.8559 (n=21)\n",
      "Method 3 - Top-22 → RF + RandomForest: Best R2 = 0.8242 (n=3)\n",
      "Method 3 - Top-22 → RF + XGBoost: Best R2 = 0.8242 (n=14)\n",
      "Method 3 - Top-22 → RFE_RF + LinearRegression: Best R2 = 0.8559 (n=21)\n",
      "Method 3 - Top-22 → RFE_RF + RandomForest: Best R2 = 0.8185 (n=17)\n",
      "Method 3 - Top-22 → RFE_RF + XGBoost: Best R2 = 0.8296 (n=16)\n",
      "Method 3 - Top-22 → XGB + LinearRegression: Best R2 = 0.8545 (n=21)\n",
      "Method 3 - Top-22 → XGB + RandomForest: Best R2 = 0.8133 (n=20)\n",
      "Method 3 - Top-22 → XGB + XGBoost: Best R2 = 0.8324 (n=20)\n",
      "Method 3 - Top-22 → RF_XGB + LinearRegression: Best R2 = 0.8543 (n=3)\n",
      "Method 3 - Top-22 → RF_XGB + RandomForest: Best R2 = 0.8144 (n=18)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ---------------------\n",
    "# Load config & setup output\n",
    "# ---------------------\n",
    "with open(\"config.json\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "dataset_slug = re.sub(r'\\W+', '_', os.path.splitext(os.path.basename(CONFIG[\"dataset_path\"]))[0])\n",
    "output_dir = os.path.join(\"metrics\", dataset_slug)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------\n",
    "# Load prediction data\n",
    "# ---------------------\n",
    "cv_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/{dataset_slug}/cv_train_predictions.csv\")\n",
    "test_df = pd.read_csv(f\"{CONFIG['predictions_dir']}/{dataset_slug}/all_predictions.csv\")\n",
    "metrics_df = pd.read_csv(f\"metrics/{dataset_slug}/{CONFIG['results_csv_path']}\")\n",
    "\n",
    "X_train = cv_df.drop(columns=[\"true\"])\n",
    "y_train = cv_df[\"true\"]\n",
    "X_test = test_df.drop(columns=[\"true\"])\n",
    "y_test = test_df[\"true\"]\n",
    "model_columns = X_train.columns.tolist()\n",
    "\n",
    "# ---------------------\n",
    "# Feature Selection Methods\n",
    "# ---------------------\n",
    "def fs_rf(X, y, n):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    scores = rf.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rfe_rf(X, y, n):\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=42), n_features_to_select=n)\n",
    "    rfe.fit(X, y)\n",
    "    return X.columns[rfe.support_]\n",
    "\n",
    "def fs_xgb(X, y, n):\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    xgb.fit(X, y)\n",
    "    scores = xgb.feature_importances_\n",
    "    return X.columns[np.argsort(scores)[::-1][:n]]\n",
    "\n",
    "def fs_rf_xgb(X, y, n):\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    xgb.fit(X, y)\n",
    "    avg_scores = (rf.feature_importances_ + xgb.feature_importances_) / 2\n",
    "    return X.columns[np.argsort(avg_scores)[::-1][:n]]\n",
    "\n",
    "fs_methods = {\n",
    "    \"RF\": fs_rf,\n",
    "    \"RFE_RF\": fs_rfe_rf,\n",
    "    \"XGB\": fs_xgb,\n",
    "    \"RF_XGB\": fs_rf_xgb\n",
    "}\n",
    "\n",
    "# Meta-models\n",
    "meta_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "    \"XGBoost\": XGBRegressor(random_state=CONFIG[\"random_state\"])\n",
    "}\n",
    "\n",
    "# ---------------------\n",
    "# Loop over Top-K values\n",
    "# ---------------------\n",
    "max_models = len(metrics_df)\n",
    "step = 10\n",
    "k_values = list(range(10, max_models, step))\n",
    "if max_models not in k_values:\n",
    "    k_values.append(max_models)\n",
    "\n",
    "best_results = []\n",
    "\n",
    "print(\"\\n=== Evaluating Methods 1, 2, and 3 across Top-K values ===\")\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n========== Top-{k} Models ==========\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 1: Random Averaging\n",
    "    # ---------------------\n",
    "    random_cols = random.sample(model_columns, k)\n",
    "    y_pred_test_random = X_test[random_cols].mean(axis=1)\n",
    "    r2_random = r2_score(y_test, y_pred_test_random)\n",
    "    print(f\"Method 1 - Random {k}: R2 = {r2_random:.4f}\")\n",
    "\n",
    "    best_results.append({\n",
    "        \"Dataset\": dataset_slug,\n",
    "        \"Method\": \"RandomAverage\",\n",
    "        \"Top_K\": k,\n",
    "        \"Meta_Model\": \"None\",\n",
    "        \"FS_Method\": \"None\",\n",
    "        \"Best_n\": k,\n",
    "        \"R2\": r2_random,\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred_test_random),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred_test_random),\n",
    "        \"Selected_Features\": random_cols\n",
    "    })\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 2: Top-K Averaging\n",
    "    # ---------------------\n",
    "    top_k_models = metrics_df.sort_values(\"CV_R2\", ascending=False).head(k)\n",
    "    top_k_cols = [\n",
    "        f\"pred_{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "        for _, row in top_k_models.iterrows()\n",
    "    ]\n",
    "\n",
    "    y_pred_test_top = X_test[top_k_cols].mean(axis=1)\n",
    "    r2_top = r2_score(y_test, y_pred_test_top)\n",
    "    print(f\"Method 2 - Top {k}: R2 = {r2_top:.4f}\")\n",
    "\n",
    "    best_results.append({\n",
    "        \"Dataset\": dataset_slug,\n",
    "        \"Method\": \"TopKAverage\",\n",
    "        \"Top_K\": k,\n",
    "        \"Meta_Model\": \"None\",\n",
    "        \"FS_Method\": \"None\",\n",
    "        \"Best_n\": k,\n",
    "        \"R2\": r2_top,\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred_test_top),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred_test_top),\n",
    "        \"Selected_Features\": top_k_cols\n",
    "    })\n",
    "\n",
    "    # ---------------------\n",
    "    # Method 3: FS + Meta Models\n",
    "    # ---------------------\n",
    "    X_train_top = X_train[top_k_cols]\n",
    "    X_test_top = X_test[top_k_cols]\n",
    "\n",
    "    for fs_name, fs_func in fs_methods.items():\n",
    "        for model_name, model in meta_models.items():\n",
    "            best_r2 = -np.inf\n",
    "            best_n = None\n",
    "            best_metrics = {}\n",
    "\n",
    "            for n in range(2, k + 1):\n",
    "                if n > len(top_k_cols):\n",
    "                    break\n",
    "                selected_feats = fs_func(X_train_top, y_train, n=n)\n",
    "                X_train_fs = X_train_top[selected_feats]\n",
    "                X_test_fs = X_test_top[selected_feats]\n",
    "\n",
    "                model.fit(X_train_fs, y_train)\n",
    "                y_pred = model.predict(X_test_fs)\n",
    "\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_n = n\n",
    "                    best_metrics = {\n",
    "                        \"R2\": r2,\n",
    "                        \"MSE\": mse,\n",
    "                        \"MAE\": mae,\n",
    "                        \"Selected_Features\": list(selected_feats)\n",
    "                    }\n",
    "\n",
    "            print(f\"Method 3 - Top-{k} → {fs_name} + {model_name}: Best R2 = {best_r2:.4f} (n={best_n})\")\n",
    "\n",
    "            best_results.append({\n",
    "                \"Dataset\": dataset_slug,\n",
    "                \"Method\": \"FS_MetaModel\",\n",
    "                \"Top_K\": k,\n",
    "                \"Meta_Model\": model_name,\n",
    "                \"FS_Method\": fs_name,\n",
    "                \"Best_n\": best_n,\n",
    "                **best_metrics\n",
    "            })\n",
    "\n",
    "# ---------------------\n",
    "# Save Results\n",
    "# ---------------------\n",
    "results_df = pd.DataFrame(best_results)\n",
    "results_df.to_csv(os.path.join(output_dir, \"best_meta_model_results.csv\"), index=False)\n",
    "print(f\"\\n Saved results to: {output_dir}/best_meta_model_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be003c1-9db5-4d0a-8807-6a67d0bab8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
