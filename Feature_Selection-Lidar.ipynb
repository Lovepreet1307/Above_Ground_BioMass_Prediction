{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ba0576-a531-41d3-a447-2515af6d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config-lidar.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(CONFIG[\"predictions_dir\"], exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess data\"\"\"\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y, \n",
    "                          test_size=CONFIG[\"test_size\"],\n",
    "                          random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    \"\"\"Get feature rankings using all 4 FS methods\"\"\"\n",
    "    rankings = {}\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rankings['RF'] = rf.feature_importances_\n",
    "    \n",
    "    # RFE-RF\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]), \n",
    "             n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rankings['RFE_RF'] = rfe.ranking_\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb = XGBRegressor(n_estimators=300,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    rankings['XGB'] = xgb.feature_importances_\n",
    "    \n",
    "    # RF-XGB Hybrid\n",
    "    hybrid = (rankings['RF'] + rankings['XGB']) / 2\n",
    "    rankings['RF_XGB'] = hybrid\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    results = []\n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    \n",
    "    # Initialize DataFrame to store CV predictions for training data\n",
    "    cv_train_preds = pd.DataFrame({'true': y_train.reset_index(drop=True)})\n",
    "    \n",
    "    for fs_name, scores in feature_rankings.items():\n",
    "        features = X_train.columns[np.argsort(scores)[::-1]]\n",
    "        \n",
    "        for model_name, Model in [('RF', RandomForestRegressor), \n",
    "                                 ('XGB', XGBRegressor)]:\n",
    "            best_metrics = {'cv_r2': -np.inf, 'cv_mse': np.inf, 'cv_mae': np.inf}\n",
    "            best_k = 0\n",
    "            \n",
    "            # Initialize array to store CV predictions for this model\n",
    "            cv_preds = np.zeros(len(y_train))\n",
    "            \n",
    "            # Feature subset evaluation\n",
    "            for k in range(1, len(features) + 1):\n",
    "                cv_r2, cv_mse, cv_mae = [], [], []\n",
    "                \n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][features[:k]], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][features[:k]])\n",
    "                    \n",
    "                    # Store predictions for this fold\n",
    "                    cv_preds[val_idx] = preds\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    cv_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "                    cv_mse.append(mean_squared_error(y_train.iloc[val_idx], preds))\n",
    "                    cv_mae.append(mean_absolute_error(y_train.iloc[val_idx], preds))\n",
    "                \n",
    "                mean_r2 = np.mean(cv_r2)\n",
    "                mean_mse = np.mean(cv_mse)\n",
    "                mean_mae = np.mean(cv_mae)\n",
    "                \n",
    "                # Use configured metric for selection\n",
    "                if CONFIG[\"metric\"] == 'r2' and mean_r2 > best_metrics['cv_r2']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "                elif CONFIG[\"metric\"] in ['mse', 'neg_mean_squared_error'] and mean_mse < best_metrics['cv_mse']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "                elif CONFIG[\"metric\"] == 'mae' and mean_mae < best_metrics['cv_mae']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "            \n",
    "            # Save CV predictions for this model\n",
    "            model_id = f\"{fs_name}_{model_name}\"\n",
    "            cv_train_preds[f'dataset2_pred_{model_id}'] = cv_preds\n",
    "            \n",
    "            # Final model training\n",
    "            final_model = Model(random_state=CONFIG[\"random_state\"])\n",
    "            final_model.fit(X_train[features[:best_k]], y_train)\n",
    "            \n",
    "            # Generate predictions\n",
    "            test_pred = final_model.predict(X_test[features[:best_k]])\n",
    "            \n",
    "            # Save artifacts\n",
    "            joblib.dump(final_model, f'models/Save_Model_{model_id}.pkl')\n",
    "            with open(f'features/Model_Features_{model_id}.json', 'w') as f:\n",
    "                json.dump(features[:best_k].tolist(), f)\n",
    "            \n",
    "            # Store results and predictions\n",
    "            combined_preds[f'pred_{model_id}'] = test_pred\n",
    "            test_metrics = {\n",
    "                'test_r2': r2_score(y_test, test_pred),\n",
    "                'test_mse': mean_squared_error(y_test, test_pred),\n",
    "                'test_mae': mean_absolute_error(y_test, test_pred)\n",
    "            }\n",
    "            \n",
    "            results.append({\n",
    "                'FS_Method': fs_name,\n",
    "                'Model': model_name,\n",
    "                'CV_R2': best_metrics['cv_r2'],\n",
    "                'CV_MSE': best_metrics['cv_mse'],\n",
    "                'CV_MAE': best_metrics['cv_mae'],\n",
    "                **test_metrics,\n",
    "                'Features_Used': best_k\n",
    "            })\n",
    "    \n",
    "    # Save all outputs\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    metrics_df.to_csv(f\"metrics/{CONFIG['results_csv_path']}\", index=False)\n",
    "    combined_preds.to_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\", index=False)\n",
    "    \n",
    "    # Save CV predictions for training data\n",
    "    cv_train_preds.to_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462f7dc1-31aa-4e96-9e16-fe1ab09a518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1488.2767491625855, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3413.8439893806353, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4231.782423964003, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4490.540854509687, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4517.447216508444, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4454.9255701766815, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4359.831404114841, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4254.236663669348, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4284.2017290769145, tolerance: 476.96324379088173\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1364.1617982864846, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4814.569317914429, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5809.340238579083, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5779.200386639684, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5581.779247510713, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5416.169500150951, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5269.669862706098, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4411.775620070868, tolerance: 454.34789204976107\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 796.333709590137, tolerance: 451.38125752725267\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 981.015064774896, tolerance: 451.38125752725267\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 991.0936738920864, tolerance: 451.38125752725267\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1340.1821015605237, tolerance: 451.38125752725267\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4623.3226821542485, tolerance: 451.38125752725267\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 965.8982262074715, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1374.9645482131746, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1464.490058519179, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3026.631446810672, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7392.94238570903, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11066.178420967888, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13127.874923719559, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13853.44274251, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13808.357757231803, tolerance: 490.66452529794685\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.592e+02, tolerance: 5.771e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second-stage feature selection, model training, and test evaluation completed successfully. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load configuration\n",
    "with open('config-lidar.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Load second-stage training data (predictions from first-stage models)\n",
    "cv_train_preds = pd.read_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\")\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = cv_train_preds.drop(columns=['true'])  # Use model predictions as features\n",
    "y = cv_train_preds['true']  # True target values\n",
    "\n",
    "# Split into second-stage Training (80%), Validation (10%), and Test (10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=CONFIG[\"random_state\"], shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=CONFIG[\"random_state\"], shuffle=True)\n",
    "\n",
    "# Standardize features (fit only on train, transform val & test)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "### --- Feature Selection for Second Stage --- ###\n",
    "def select_features_second_stage(X_train, y_train):\n",
    "    selected_features = {}\n",
    "\n",
    "    # 1. Random Forest Feature Selection\n",
    "    rf_selector = RandomForestRegressor(n_estimators=100, random_state=CONFIG[\"random_state\"])\n",
    "    rf_selector.fit(X_train, y_train)\n",
    "    feature_importances = rf_selector.feature_importances_\n",
    "    selected_features[\"RF\"] = X_train.columns[feature_importances > np.percentile(feature_importances, 50)]\n",
    "\n",
    "    # 2. RFE with Random Forest\n",
    "    rfe_selector = RFE(RandomForestRegressor(n_estimators=50, random_state=CONFIG[\"random_state\"]), n_features_to_select=10)\n",
    "    rfe_selector.fit(X_train, y_train)\n",
    "    selected_features[\"RFE\"] = X_train.columns[rfe_selector.support_]\n",
    "\n",
    "    # 3. Mutual Information-based Selection\n",
    "    mi_scores = mutual_info_regression(X_train, y_train)\n",
    "    selected_features[\"Mutual_Info\"] = X_train.columns[mi_scores > np.percentile(mi_scores, 50)]\n",
    "\n",
    "    # 4. LASSO Feature Selection\n",
    "    lasso = LassoCV(cv=5, random_state=CONFIG[\"random_state\"])\n",
    "    lasso.fit(X_train, y_train)\n",
    "    selected_features[\"LASSO\"] = X_train.columns[np.abs(lasso.coef_) > 0]\n",
    "\n",
    "    # Save selected features\n",
    "    with open(\"metrics/selected_features_second_stage.json\", \"w\") as f:\n",
    "        json.dump({k: list(v) for k, v in selected_features.items()}, f)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Select features using the best method (Random Forest in this case)\n",
    "selected_features_second_stage = select_features_second_stage(X_train, y_train)\n",
    "\n",
    "# Apply selected features to train, validation, and test sets\n",
    "X_train_selected = X_train[selected_features_second_stage[\"RF\"]]\n",
    "X_val_selected = X_val[selected_features_second_stage[\"RF\"]]\n",
    "X_test_selected = X_test[selected_features_second_stage[\"RF\"]]\n",
    "\n",
    "### --- Model Training & Evaluation --- ###\n",
    "def cross_val_metrics(model, X, y, kf):\n",
    "    \"\"\"Performs cross-validation and returns R2, MAE, and MSE scores.\"\"\"\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    mae_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    return np.mean(r2_scores), np.mean(mae_scores), np.mean(mse_scores)\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Trains models, evaluates on validation set, and returns trained models with metrics.\"\"\"\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "        \"Linear Regression\": LinearRegression()\n",
    "    }\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    rf_model = GridSearchCV(RandomForestRegressor(random_state=CONFIG[\"random_state\"]), param_grid, cv=5, scoring='r2')\n",
    "\n",
    "    kf = KFold(n_splits=CONFIG[\"cv_folds\"], shuffle=True, random_state=CONFIG[\"random_state\"])\n",
    "    results = []\n",
    "\n",
    "    trained_models = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Cross-validation scores\n",
    "        cv_r2, cv_mae, cv_mse = cross_val_metrics(model, X_train, y_train, kf)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[model_name] = model\n",
    "\n",
    "        # Validation scores\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_mae = mean_absolute_error(y_val, val_preds)\n",
    "        val_mse = mean_squared_error(y_val, val_preds)\n",
    "        val_r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'CV R2': cv_r2,\n",
    "            'CV MAE': cv_mae,\n",
    "            'CV MSE': cv_mse,\n",
    "            'Validation R2': val_r2,\n",
    "            'Validation MAE': val_mae,\n",
    "            'Validation MSE': val_mse\n",
    "        })\n",
    "\n",
    "    return trained_models, pd.DataFrame(results)\n",
    "\n",
    "# Train models and evaluate on validation set\n",
    "trained_models, results_df = train_and_evaluate_models(X_train_selected, X_val_selected, y_train, y_val)\n",
    "\n",
    "# Save validation results\n",
    "results_df.to_csv(\"metrics/second_stage_model_results_lidar.csv\", index=False)\n",
    "\n",
    "### --- Test Set Evaluation --- ###\n",
    "def evaluate_on_test(models, X_test, y_test):\n",
    "    \"\"\"Evaluates trained models on test data and saves results.\"\"\"\n",
    "    test_results = []\n",
    "    for model_name, model in models.items():\n",
    "        test_preds = model.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, test_preds)\n",
    "        test_mae = mean_absolute_error(y_test, test_preds)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "\n",
    "        test_results.append({\n",
    "            'Model': model_name,\n",
    "            'Test R2': test_r2,\n",
    "            'Test MAE': test_mae,\n",
    "            'Test MSE': test_mse\n",
    "        })\n",
    "\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "    test_results_df.to_csv(\"metrics/second_stage_test_results_lidar.csv\", index=False)\n",
    "\n",
    "# Evaluate models on test data\n",
    "evaluate_on_test(trained_models, X_test_selected, y_test)\n",
    "\n",
    "### --- Save Final Models --- ###\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "for model_name, model in trained_models.items():\n",
    "    joblib.dump(model, f\"models/{model_name.replace(' ', '_').lower()}_second_stage_lidar.pkl\")\n",
    "\n",
    "print(\"Second-stage feature selection, model training, and test evaluation completed successfully. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c67347-59e0-469d-ac76-2ceff68562cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second-stage modeling completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Load the CV predictions for training data\n",
    "cv_train_preds = pd.read_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\")\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = cv_train_preds.drop(columns=['true'])  # Use all prediction columns as features\n",
    "y = cv_train_preds['true']  # True values as target\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "# Initialize KFold for cross-validation using cv_folds from config\n",
    "kf = KFold(n_splits=CONFIG[\"cv_folds\"], shuffle=True, random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestRegressor(random_state=CONFIG[\"random_state\"])\n",
    "gb_model = GradientBoostingRegressor(random_state=CONFIG[\"random_state\"])\n",
    "lr_model = LinearRegression()  # Linear Regression\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "rf_cv_mae = -cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
    "rf_cv_mse = -cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform cross-validation for Gradient Boosting\n",
    "gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "gb_cv_mae = -cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
    "gb_cv_mse = -cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform cross-validation for Linear Regression\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "lr_cv_mae = -cross_val_score(lr_model, X_train, y_train, cv=kf, scoring='neg_mean_absolute_error')\n",
    "lr_cv_mse = -cross_val_score(lr_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the models on the full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)  # Train Linear Regression\n",
    "\n",
    "# Evaluate Random Forest on validation set\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "rf_mae = mean_absolute_error(y_val, rf_preds)\n",
    "rf_mse = mean_squared_error(y_val, rf_preds)\n",
    "rf_r2 = r2_score(y_val, rf_preds)\n",
    "\n",
    "# Evaluate Gradient Boosting on validation set\n",
    "gb_preds = gb_model.predict(X_val)\n",
    "gb_mae = mean_absolute_error(y_val, gb_preds)\n",
    "gb_mse = mean_squared_error(y_val, gb_preds)\n",
    "gb_r2 = r2_score(y_val, gb_preds)\n",
    "\n",
    "# Evaluate Linear Regression on validation set\n",
    "lr_preds = lr_model.predict(X_val)\n",
    "lr_mae = mean_absolute_error(y_val, lr_preds)\n",
    "lr_mse = mean_squared_error(y_val, lr_preds)\n",
    "lr_r2 = r2_score(y_val, lr_preds)\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'Linear Regression'],\n",
    "    'CV R2 Mean': [np.mean(rf_cv_scores), np.mean(gb_cv_scores), np.mean(lr_cv_scores)],\n",
    "    'CV MAE Mean': [np.mean(rf_cv_mae), np.mean(gb_cv_mae), np.mean(lr_cv_mae)],\n",
    "    'CV MSE Mean': [np.mean(rf_cv_mse), np.mean(gb_cv_mse), np.mean(lr_cv_mse)],\n",
    "    'Validation R2': [rf_r2, gb_r2, lr_r2],\n",
    "    'Validation MAE': [rf_mae, gb_mae, lr_mae],\n",
    "    'Validation MSE': [rf_mse, gb_mse, lr_mse]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"metrics/second_stage_model_results.csv\", index=False)  # Save to metrics/ directory\n",
    "\n",
    "# Save the second-stage models\n",
    "joblib.dump(rf_model, \"models/second_stage_rf_model.pkl\")  # Save to models/ directory\n",
    "joblib.dump(gb_model, \"models/second_stage_gb_model.pkl\")  # Save to models/ directory\n",
    "joblib.dump(lr_model, \"models/second_stage_lr_model.pkl\")  # Save Linear Regression model\n",
    "\n",
    "print(\"Second-stage modeling completed. Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4dc66-6542-4cdc-8eb6-34386aa6b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
