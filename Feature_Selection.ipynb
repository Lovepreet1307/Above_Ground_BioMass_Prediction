{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ba0576-a531-41d3-a447-2515af6d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(CONFIG[\"predictions_dir\"], exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)  # For trained models\n",
    "os.makedirs(\"features\", exist_ok=True)  # For feature selection results\n",
    "os.makedirs(\"metrics\", exist_ok=True)  # For metric files\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess data\"\"\"\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y, \n",
    "                          test_size=CONFIG[\"test_size\"],\n",
    "                          random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    \"\"\"Get feature rankings using all 4 FS methods\"\"\"\n",
    "    rankings = {}\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor(random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rankings['RF'] = rf.feature_importances_\n",
    "    \n",
    "    # RFE-RF\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]), \n",
    "             n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rankings['RFE_RF'] = rfe.ranking_\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb = XGBRegressor(random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    rankings['XGB'] = xgb.feature_importances_\n",
    "    \n",
    "    # RF-XGB Hybrid\n",
    "    hybrid = (rankings['RF'] + rankings['XGB']) / 2\n",
    "    rankings['RF_XGB'] = hybrid\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    results = []\n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    \n",
    "    for fs_name, scores in feature_rankings.items():\n",
    "        features = X_train.columns[np.argsort(scores)[::-1]]\n",
    "        \n",
    "        for model_name, Model in [('RF', RandomForestRegressor), \n",
    "                                 ('XGB', XGBRegressor)]:\n",
    "            best_metrics = {'cv_r2': -np.inf, 'cv_mse': np.inf, 'cv_mae': np.inf}\n",
    "            best_k = 0\n",
    "            \n",
    "            # Feature subset evaluation\n",
    "            for k in range(1, len(features)+1):\n",
    "                cv_r2, cv_mse, cv_mae = [], [], []\n",
    "                \n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][features[:k]], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][features[:k]])\n",
    "                    \n",
    "                    cv_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "                    cv_mse.append(mean_squared_error(y_train.iloc[val_idx], preds))\n",
    "                    cv_mae.append(mean_absolute_error(y_train.iloc[val_idx], preds))\n",
    "                \n",
    "                mean_r2 = np.mean(cv_r2)\n",
    "                mean_mse = np.mean(cv_mse)\n",
    "                mean_mae = np.mean(cv_mae)\n",
    "                \n",
    "                # Use configured metric for selection\n",
    "                if CONFIG[\"metric\"] == 'r2' and mean_r2 > best_metrics['cv_r2']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "                elif CONFIG[\"metric\"] in ['mse', 'neg_mean_squared_error'] and mean_mse < best_metrics['cv_mse']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "                elif CONFIG[\"metric\"] == 'mae' and mean_mae < best_metrics['cv_mae']:\n",
    "                    best_metrics = {'cv_r2': mean_r2, 'cv_mse': mean_mse, 'cv_mae': mean_mae}\n",
    "                    best_k = k\n",
    "            \n",
    "            # Final model training\n",
    "            final_model = Model(random_state=CONFIG[\"random_state\"])\n",
    "            final_model.fit(X_train[features[:best_k]], y_train)\n",
    "            \n",
    "            # Generate predictions\n",
    "            test_pred = final_model.predict(X_test[features[:best_k]])\n",
    "            model_id = f\"{fs_name}_{model_name}\"\n",
    "            \n",
    "            # Save artifacts\n",
    "            joblib.dump(final_model, f'models/Model_{model_id}.pkl')\n",
    "            with open(f'features/Features_{model_id}.json', 'w') as f:\n",
    "                json.dump(features[:best_k].tolist(), f)\n",
    "            \n",
    "            # Store results and predictions\n",
    "            combined_preds[f'pred_{model_id}'] = test_pred\n",
    "            test_metrics = {\n",
    "                'test_r2': r2_score(y_test, test_pred),\n",
    "                'test_mse': mean_squared_error(y_test, test_pred),\n",
    "                'test_mae': mean_absolute_error(y_test, test_pred)\n",
    "            }\n",
    "            \n",
    "            results.append({\n",
    "                'FS_Method': fs_name,\n",
    "                'Model': model_name,\n",
    "                'CV_R2': best_metrics['cv_r2'],\n",
    "                'CV_MSE': best_metrics['cv_mse'],\n",
    "                'CV_MAE': best_metrics['cv_mae'],\n",
    "                **test_metrics,\n",
    "                'Features_Used': best_k\n",
    "            })\n",
    "    \n",
    "    # Save all outputs\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    metrics_df.to_csv(f\"metrics/{CONFIG['results_csv_path']}\", index=False)\n",
    "    combined_preds.to_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c67347-59e0-469d-ac76-2ceff68562cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
