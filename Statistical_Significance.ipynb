{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedd6738-bc1e-4ca0-aecd-91628cad4e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STATISTICAL ANALYSIS SUITE\n",
      "================================================================================\n",
      "\n",
      "[1] Loading data...\n",
      "⚠ No baseline metrics found\n",
      "✓ Stage 1 configurations: 54\n",
      "✓ Stage 2 configurations: 36\n",
      "✓ Stage 1 fold records: 540\n",
      "✓ Stage 2 fold records: 360\n",
      "\n",
      "================================================================================\n",
      "[2] STATISTICAL SIGNIFICANCE TESTING\n",
      "================================================================================\n",
      "\n",
      "2.1: One-Sample t-test (H0: R² = 0)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best Stage 1: RFE_RF_RF_Top5\n",
      "  Mean R²: 0.7325 ± 0.0776\n",
      "  t-statistic: 29.8590\n",
      "  p-value (one-tailed): 0.000000\n",
      "  Result: ✓ Significant (p < 0.05)\n",
      "\n",
      "Best Stage 2: TopKAverage_TopK50\n",
      "  Mean R²: 0.7508 ± 0.0674\n",
      "  t-statistic: 35.2012\n",
      "  p-value (one-tailed): 0.000000\n",
      "  Result: ✓ Significant (p < 0.05)\n",
      "\n",
      "2.2: Paired t-test (Stage 2 vs Best Stage 1)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Stage 1 R²: 0.7325 ± 0.0776\n",
      "Stage 2 R²: 0.7508 ± 0.0674\n",
      "Difference: 0.0183\n",
      "t-statistic: 0.4674\n",
      "p-value: 0.6513\n",
      "Result: ✗ No significant difference\n",
      "\n",
      "✓ Saved significance tests to: metrics/PRF_Landsat5_9/statistical_analysis/significance_tests.csv\n",
      "\n",
      "================================================================================\n",
      "[3] STABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "3.1: Coefficient of Variation (CV%)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 5 Stage 1 Models:\n",
      "  RFE_RF_RF_Top5: CV = 10.59%\n",
      "  RF_RF_Top5: CV = 10.59%\n",
      "  RF_XGB_Top7: CV = 11.98%\n",
      "  RF_RF_Top6: CV = 10.15%\n",
      "  RFE_RF_RF_Top6: CV = 10.15%\n",
      "\n",
      "Top 5 Stage 2 Models:\n",
      "  TopKAverage_TopK50: CV = 8.98%\n",
      "  FS_MetaModel_TopK40: CV = 9.83%\n",
      "  TopKAverage_TopK40: CV = 9.41%\n",
      "  RandomAverage_TopK20: CV = 8.90%\n",
      "  TopKAverage_TopK30: CV = 9.33%\n",
      "\n",
      "Average CV% - Stage 1: 10.69%\n",
      "Average CV% - Stage 2: 9.29%\n",
      "Stability improvement: 13.1%\n",
      "\n",
      "3.2: Levene's Test for Variance Equality\n",
      "------------------------------------------------------------\n",
      "Levene's statistic: 91.2424\n",
      "p-value: 0.0000\n",
      "Result: ✓ Variances significantly different (p < 0.05)\n",
      "\n",
      "✓ Saved stability analysis to: metrics/PRF_Landsat5_9/statistical_analysis/stability_analysis.csv\n",
      "\n",
      "3.3: Creating stability visualizations...\n",
      "✓ Saved box plots to: metrics/PRF_Landsat5_9/statistical_analysis/stability_boxplots.png\n",
      "\n",
      "================================================================================\n",
      "[4] INTERPRETABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "4.1: Meta-Model Weight Analysis (24 configurations)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best LR Meta-Model:\n",
      "  Method: FS_MetaModel\n",
      "  Top-K: 40\n",
      "  FS Method: RF_XGB\n",
      "  Best n: 9\n",
      "  Test R²: 0.7673\n",
      "\n",
      "Meta-Model Weights:\n",
      "     Base_Model    Weight\n",
      " RF_XGB_RF_Top7  0.354934\n",
      "    XGB_RF_Top7  0.354934\n",
      "     RF_RF_Top6  0.305407\n",
      " RF_XGB_RF_Top2  0.213740\n",
      "    RF_XGB_Top3  0.139758\n",
      "RF_XGB_XGB_Top6  0.129611\n",
      "    XGB_RF_Top5 -0.142164\n",
      " RFE_RF_RF_Top5 -0.160113\n",
      "     RF_RF_Top5 -0.160113\n",
      "\n",
      "Intercept: -9.1740\n",
      "Sum of weights: 1.0360\n",
      "Dominant model: RF_XGB_RF_Top7 (weight=0.3549)\n",
      "\n",
      "✓ Saved weight visualization to: metrics/PRF_Landsat5_9/statistical_analysis/meta_model_weights.png\n",
      "\n",
      "4.2: Interpretation Insights\n",
      "------------------------------------------------------------\n",
      "\n",
      "Positive contributors: 6 models\n",
      "Negative contributors: 3 models (bias correction)\n",
      "Near-zero weights (<0.05): 0 models (redundant)\n",
      "\n",
      "Negative weights suggest these models correct systematic biases:\n",
      "    Base_Model    Weight\n",
      "   XGB_RF_Top5 -0.142164\n",
      "RFE_RF_RF_Top5 -0.160113\n",
      "    RF_RF_Top5 -0.160113\n",
      "\n",
      "================================================================================\n",
      "[5] GENERATING SUMMARY REPORT\n",
      "================================================================================\n",
      "================================================================================\n",
      "STATISTICAL ANALYSIS SUMMARY REPORT\n",
      "Dataset: PRF_Landsat5_9\n",
      "================================================================================\n",
      "\n",
      "--- 1. STATISTICAL SIGNIFICANCE ---\n",
      "Best Stage 1 Model: RFE_RF_RF_Top5\n",
      "  Mean CV R²: 0.7325 ± 0.0776\n",
      "  Significantly better than baseline (R²=0): YES (p < 0.05)\n",
      "\n",
      "Best Stage 2 Model: TopKAverage_TopK50\n",
      "  Mean CV R²: 0.7508 ± 0.0674\n",
      "  Significantly better than baseline (R²=0): YES (p < 0.05)\n",
      "\n",
      "Stage 2 vs Stage 1 (Paired t-test):\n",
      "  Difference in R²: 0.0183\n",
      "  p-value: 0.6513\n",
      "  Significantly different: NO\n",
      "\n",
      "--- 2. STABILITY ANALYSIS ---\n",
      "Average CV% - Stage 1: 10.69%\n",
      "Average CV% - Stage 2: 9.29%\n",
      "Stability improvement: 13.1%\n",
      "\n",
      "--- 3. INTERPRETABILITY ---\n",
      "Meta-Model Type: Linear Regression\n",
      "Number of base learners: 9\n",
      "Dominant model: RF_XGB_RF_Top7 (weight=0.3549)\n",
      "Positive contributors: 6\n",
      "Negative contributors: 3 (bias correction)\n",
      "Redundant models: 0\n",
      "\n",
      "✓ Saved summary report to: metrics/PRF_Landsat5_9/statistical_analysis/summary_report.txt\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "All results saved to: metrics/PRF_Landsat5_9/statistical_analysis/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Statistical Significance, Stability, and Interpretability Analysis\n",
    "Run this after Stage 1 and Stage 2 are complete\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "dataset_slug = re.sub(r'\\W+', '_', os.path.splitext(os.path.basename(CONFIG[\"dataset_path\"]))[0])\n",
    "\n",
    "# Define paths\n",
    "base_dir = os.path.join(\"metrics\", dataset_slug)\n",
    "fold_metrics_dir = os.path.join(base_dir, \"fold_wise\")\n",
    "analysis_dir = os.path.join(base_dir, \"statistical_analysis\")\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS SUITE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n[1] Loading data...\")\n",
    "\n",
    "stage1_fold_metrics = pd.read_csv(os.path.join(fold_metrics_dir, \"stage1_fold_metrics.csv\"))\n",
    "stage2_fold_metrics = pd.read_csv(os.path.join(fold_metrics_dir, \"stage2_fold_metrics.csv\"))\n",
    "stage1_results = pd.read_csv(os.path.join(base_dir, CONFIG['results_csv_path']))\n",
    "stage2_results = pd.read_csv(os.path.join(base_dir, \"best_meta_model_results.csv\"))\n",
    "\n",
    "# Load baseline metrics if available\n",
    "baseline_path = os.path.join(CONFIG[\"predictions_dir\"], dataset_slug, \"baseline_model_metrics.csv\")\n",
    "if os.path.exists(baseline_path):\n",
    "    baseline_metrics = pd.read_csv(baseline_path, index_col=0)\n",
    "    print(f\"✓ Loaded baseline metrics\")\n",
    "else:\n",
    "    baseline_metrics = None\n",
    "    print(\"⚠ No baseline metrics found\")\n",
    "\n",
    "print(f\"✓ Stage 1 configurations: {len(stage1_results)}\")\n",
    "print(f\"✓ Stage 2 configurations: {len(stage2_results)}\")\n",
    "print(f\"✓ Stage 1 fold records: {len(stage1_fold_metrics)}\")\n",
    "print(f\"✓ Stage 2 fold records: {len(stage2_fold_metrics)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. STATISTICAL SIGNIFICANCE TESTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2] STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "significance_results = []\n",
    "\n",
    "# 2.1: Test if models are significantly better than baseline (R² > 0)\n",
    "print(\"\\n2.1: One-Sample t-test (H0: R² = 0)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Best Stage 1 model\n",
    "best_stage1 = stage1_results.loc[stage1_results['Test_R2'].idxmax()]\n",
    "stage1_config = f\"{best_stage1['FS_Method']}_{best_stage1['Model']}_Top{int(best_stage1['Features_Used'])}\"\n",
    "stage1_r2_folds = stage1_fold_metrics[\n",
    "    (stage1_fold_metrics['FS_Method'] == best_stage1['FS_Method']) &\n",
    "    (stage1_fold_metrics['Model'] == best_stage1['Model']) &\n",
    "    (stage1_fold_metrics['Features_Used'] == best_stage1['Features_Used'])\n",
    "]['R2'].values\n",
    "\n",
    "t_stat, p_value = stats.ttest_1samp(stage1_r2_folds, 0)\n",
    "p_value_one_tailed = p_value / 2\n",
    "\n",
    "print(f\"\\nBest Stage 1: {stage1_config}\")\n",
    "print(f\"  Mean R²: {np.mean(stage1_r2_folds):.4f} ± {np.std(stage1_r2_folds, ddof=1):.4f}\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value (one-tailed): {p_value_one_tailed:.6f}\")\n",
    "print(f\"  Result: {'✓ Significant (p < 0.05)' if p_value_one_tailed < 0.05 else '✗ Not significant'}\")\n",
    "\n",
    "significance_results.append({\n",
    "    'Test': 'One-sample t-test',\n",
    "    'Comparison': 'Best Stage 1 vs R²=0',\n",
    "    'Model': stage1_config,\n",
    "    'Mean_R2': np.mean(stage1_r2_folds),\n",
    "    'Std_R2': np.std(stage1_r2_folds, ddof=1),\n",
    "    't_statistic': t_stat,\n",
    "    'p_value': p_value_one_tailed,\n",
    "    'Significant': p_value_one_tailed < 0.05\n",
    "})\n",
    "\n",
    "# Best Stage 2 model\n",
    "best_stage2 = stage2_results.loc[stage2_results['Test_R2'].idxmax()]\n",
    "stage2_config = f\"{best_stage2['Method']}_TopK{int(best_stage2['Top_K'])}\"\n",
    "\n",
    "if best_stage2['Method'] == 'FS_MetaModel':\n",
    "    stage2_r2_folds = stage2_fold_metrics[\n",
    "        (stage2_fold_metrics['Method'] == best_stage2['Method']) &\n",
    "        (stage2_fold_metrics['Meta_Model'] == best_stage2['Meta_Model']) &\n",
    "        (stage2_fold_metrics['FS_Method'] == best_stage2['FS_Method']) &\n",
    "        (stage2_fold_metrics['Top_K'] == best_stage2['Top_K']) &\n",
    "        (stage2_fold_metrics['Best_n'] == best_stage2['Best_n'])\n",
    "    ]['R2'].values\n",
    "else:\n",
    "    stage2_r2_folds = stage2_fold_metrics[\n",
    "        (stage2_fold_metrics['Method'] == best_stage2['Method']) &\n",
    "        (stage2_fold_metrics['Top_K'] == best_stage2['Top_K'])\n",
    "    ]['R2'].values\n",
    "\n",
    "t_stat, p_value = stats.ttest_1samp(stage2_r2_folds, 0)\n",
    "p_value_one_tailed = p_value / 2\n",
    "\n",
    "print(f\"\\nBest Stage 2: {stage2_config}\")\n",
    "print(f\"  Mean R²: {np.mean(stage2_r2_folds):.4f} ± {np.std(stage2_r2_folds, ddof=1):.4f}\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value (one-tailed): {p_value_one_tailed:.6f}\")\n",
    "print(f\"  Result: {'✓ Significant (p < 0.05)' if p_value_one_tailed < 0.05 else '✗ Not significant'}\")\n",
    "\n",
    "significance_results.append({\n",
    "    'Test': 'One-sample t-test',\n",
    "    'Comparison': 'Best Stage 2 vs R²=0',\n",
    "    'Model': stage2_config,\n",
    "    'Mean_R2': np.mean(stage2_r2_folds),\n",
    "    'Std_R2': np.std(stage2_r2_folds, ddof=1),\n",
    "    't_statistic': t_stat,\n",
    "    'p_value': p_value_one_tailed,\n",
    "    'Significant': p_value_one_tailed < 0.05\n",
    "})\n",
    "\n",
    "# 2.2: Paired t-test: Stage 2 vs Stage 1\n",
    "print(\"\\n2.2: Paired t-test (Stage 2 vs Best Stage 1)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if len(stage1_r2_folds) == len(stage2_r2_folds):\n",
    "    t_stat, p_value = stats.ttest_rel(stage2_r2_folds, stage1_r2_folds)\n",
    "    \n",
    "    print(f\"\\nStage 1 R²: {np.mean(stage1_r2_folds):.4f} ± {np.std(stage1_r2_folds, ddof=1):.4f}\")\n",
    "    print(f\"Stage 2 R²: {np.mean(stage2_r2_folds):.4f} ± {np.std(stage2_r2_folds, ddof=1):.4f}\")\n",
    "    print(f\"Difference: {np.mean(stage2_r2_folds - stage1_r2_folds):.4f}\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "    print(f\"Result: {'✓ Significant difference (p < 0.05)' if p_value < 0.05 else '✗ No significant difference'}\")\n",
    "    \n",
    "    significance_results.append({\n",
    "        'Test': 'Paired t-test',\n",
    "        'Comparison': 'Stage 2 vs Stage 1',\n",
    "        'Model': f\"{stage2_config} vs {stage1_config}\",\n",
    "        'Mean_R2': np.mean(stage2_r2_folds - stage1_r2_folds),\n",
    "        'Std_R2': np.std(stage2_r2_folds - stage1_r2_folds, ddof=1),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'Significant': p_value < 0.05\n",
    "    })\n",
    "else:\n",
    "    print(\"⚠ Cannot perform paired t-test: fold counts don't match\")\n",
    "\n",
    "# Save significance results\n",
    "sig_df = pd.DataFrame(significance_results)\n",
    "sig_df.to_csv(os.path.join(analysis_dir, \"significance_tests.csv\"), index=False)\n",
    "print(f\"\\n✓ Saved significance tests to: {analysis_dir}/significance_tests.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. STABILITY ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3] STABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stability_results = []\n",
    "\n",
    "# 3.1: Coefficient of Variation\n",
    "print(\"\\n3.1: Coefficient of Variation (CV%)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def coefficient_of_variation(values):\n",
    "    return (np.std(values, ddof=1) / np.mean(values)) * 100\n",
    "\n",
    "# Get top 5 Stage 1 models\n",
    "top5_stage1 = stage1_results.nlargest(5, 'Test_R2')\n",
    "\n",
    "print(\"\\nTop 5 Stage 1 Models:\")\n",
    "for idx, row in top5_stage1.iterrows():\n",
    "    model_id = f\"{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "    folds = stage1_fold_metrics[\n",
    "        (stage1_fold_metrics['FS_Method'] == row['FS_Method']) &\n",
    "        (stage1_fold_metrics['Model'] == row['Model']) &\n",
    "        (stage1_fold_metrics['Features_Used'] == row['Features_Used'])\n",
    "    ]['R2'].values\n",
    "    \n",
    "    cv_pct = coefficient_of_variation(folds)\n",
    "    print(f\"  {model_id}: CV = {cv_pct:.2f}%\")\n",
    "    \n",
    "    stability_results.append({\n",
    "        'Stage': 'Stage 1',\n",
    "        'Model': model_id,\n",
    "        'Mean_R2': np.mean(folds),\n",
    "        'Std_R2': np.std(folds, ddof=1),\n",
    "        'CV_Percent': cv_pct,\n",
    "        'Min_R2': np.min(folds),\n",
    "        'Max_R2': np.max(folds),\n",
    "        'Range_R2': np.max(folds) - np.min(folds)\n",
    "    })\n",
    "\n",
    "# Average CV for Stage 1\n",
    "avg_cv_stage1 = np.mean([r['CV_Percent'] for r in stability_results if r['Stage'] == 'Stage 1'])\n",
    "\n",
    "print(\"\\nTop 5 Stage 2 Models:\")\n",
    "top5_stage2 = stage2_results.nlargest(5, 'Test_R2')\n",
    "\n",
    "for idx, row in top5_stage2.iterrows():\n",
    "    model_id = f\"{row['Method']}_TopK{int(row['Top_K'])}\"\n",
    "    \n",
    "    if row['Method'] == 'FS_MetaModel':\n",
    "        folds = stage2_fold_metrics[\n",
    "            (stage2_fold_metrics['Method'] == row['Method']) &\n",
    "            (stage2_fold_metrics['Meta_Model'] == row['Meta_Model']) &\n",
    "            (stage2_fold_metrics['FS_Method'] == row['FS_Method']) &\n",
    "            (stage2_fold_metrics['Top_K'] == row['Top_K']) &\n",
    "            (stage2_fold_metrics['Best_n'] == row['Best_n'])\n",
    "        ]['R2'].values\n",
    "    else:\n",
    "        folds = stage2_fold_metrics[\n",
    "            (stage2_fold_metrics['Method'] == row['Method']) &\n",
    "            (stage2_fold_metrics['Top_K'] == row['Top_K'])\n",
    "        ]['R2'].values\n",
    "    \n",
    "    if len(folds) > 0:\n",
    "        cv_pct = coefficient_of_variation(folds)\n",
    "        print(f\"  {model_id}: CV = {cv_pct:.2f}%\")\n",
    "        \n",
    "        stability_results.append({\n",
    "            'Stage': 'Stage 2',\n",
    "            'Model': model_id,\n",
    "            'Mean_R2': np.mean(folds),\n",
    "            'Std_R2': np.std(folds, ddof=1),\n",
    "            'CV_Percent': cv_pct,\n",
    "            'Min_R2': np.min(folds),\n",
    "            'Max_R2': np.max(folds),\n",
    "            'Range_R2': np.max(folds) - np.min(folds)\n",
    "        })\n",
    "\n",
    "avg_cv_stage2 = np.mean([r['CV_Percent'] for r in stability_results if r['Stage'] == 'Stage 2'])\n",
    "\n",
    "print(f\"\\nAverage CV% - Stage 1: {avg_cv_stage1:.2f}%\")\n",
    "print(f\"Average CV% - Stage 2: {avg_cv_stage2:.2f}%\")\n",
    "print(f\"Stability improvement: {((avg_cv_stage1 - avg_cv_stage2) / avg_cv_stage1 * 100):.1f}%\")\n",
    "\n",
    "# 3.2: Variance comparison test\n",
    "print(\"\\n3.2: Levene's Test for Variance Equality\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get all Stage 1 R2 values\n",
    "all_stage1_r2 = []\n",
    "for idx, row in stage1_results.iterrows():\n",
    "    folds = stage1_fold_metrics[\n",
    "        (stage1_fold_metrics['FS_Method'] == row['FS_Method']) &\n",
    "        (stage1_fold_metrics['Model'] == row['Model']) &\n",
    "        (stage1_fold_metrics['Features_Used'] == row['Features_Used'])\n",
    "    ]['R2'].values\n",
    "    all_stage1_r2.extend(folds)\n",
    "\n",
    "# Get all Stage 2 R2 values\n",
    "all_stage2_r2 = stage2_fold_metrics['R2'].values\n",
    "\n",
    "stat, p_value = stats.levene(all_stage1_r2, all_stage2_r2)\n",
    "\n",
    "print(f\"Levene's statistic: {stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Result: {'✓ Variances significantly different (p < 0.05)' if p_value < 0.05 else '✗ No significant difference'}\")\n",
    "\n",
    "# Save stability results\n",
    "stab_df = pd.DataFrame(stability_results)\n",
    "stab_df.to_csv(os.path.join(analysis_dir, \"stability_analysis.csv\"), index=False)\n",
    "print(f\"\\n✓ Saved stability analysis to: {analysis_dir}/stability_analysis.csv\")\n",
    "\n",
    "# 3.3: Visualization - Box plots\n",
    "print(\"\\n3.3: Creating stability visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stage 1 box plot\n",
    "stage1_data = []\n",
    "stage1_labels = []\n",
    "for idx, row in top5_stage1.iterrows():\n",
    "    folds = stage1_fold_metrics[\n",
    "        (stage1_fold_metrics['FS_Method'] == row['FS_Method']) &\n",
    "        (stage1_fold_metrics['Model'] == row['Model']) &\n",
    "        (stage1_fold_metrics['Features_Used'] == row['Features_Used'])\n",
    "    ]['R2'].values\n",
    "    stage1_data.append(folds)\n",
    "    stage1_labels.append(f\"{row['Model']}\\n{row['FS_Method']}\")\n",
    "\n",
    "bp1 = axes[0].boxplot(stage1_data, labels=stage1_labels, patch_artist=True)\n",
    "for patch in bp1['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "axes[0].set_title('Stage 1: Top 5 Models - R² Distribution')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Stage 2 box plot\n",
    "stage2_data = []\n",
    "stage2_labels = []\n",
    "for idx, row in top5_stage2.iterrows():\n",
    "    if row['Method'] == 'FS_MetaModel':\n",
    "        folds = stage2_fold_metrics[\n",
    "            (stage2_fold_metrics['Method'] == row['Method']) &\n",
    "            (stage2_fold_metrics['Meta_Model'] == row['Meta_Model']) &\n",
    "            (stage2_fold_metrics['FS_Method'] == row['FS_Method']) &\n",
    "            (stage2_fold_metrics['Top_K'] == row['Top_K']) &\n",
    "            (stage2_fold_metrics['Best_n'] == row['Best_n'])\n",
    "        ]['R2'].values\n",
    "    else:\n",
    "        folds = stage2_fold_metrics[\n",
    "            (stage2_fold_metrics['Method'] == row['Method']) &\n",
    "            (stage2_fold_metrics['Top_K'] == row['Top_K'])\n",
    "        ]['R2'].values\n",
    "    \n",
    "    if len(folds) > 0:\n",
    "        stage2_data.append(folds)\n",
    "        stage2_labels.append(f\"{row['Method']}\\nK={int(row['Top_K'])}\")\n",
    "\n",
    "bp2 = axes[1].boxplot(stage2_data, labels=stage2_labels, patch_artist=True)\n",
    "for patch in bp2['boxes']:\n",
    "    patch.set_facecolor('lightgreen')\n",
    "axes[1].set_title('Stage 2: Top 5 Models - R² Distribution')\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(analysis_dir, \"stability_boxplots.png\"), dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved box plots to: {analysis_dir}/stability_boxplots.png\")\n",
    "plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INTERPRETABILITY ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4] INTERPRETABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Only analyze if Stage 2 used Linear Regression meta-model\n",
    "lr_models = stage2_results[stage2_results['Meta_Model'] == 'LinearRegression']\n",
    "\n",
    "if len(lr_models) == 0:\n",
    "    print(\"⚠ No Linear Regression meta-models found in Stage 2\")\n",
    "else:\n",
    "    print(f\"\\n4.1: Meta-Model Weight Analysis ({len(lr_models)} configurations)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Load Stage 1 predictions\n",
    "    cv_train_preds = pd.read_csv(\n",
    "        os.path.join(CONFIG[\"predictions_dir\"], dataset_slug, \"cv_train_predictions.csv\")\n",
    "    )\n",
    "    \n",
    "    X_train_stage1 = cv_train_preds.drop(columns=[\"true\"])\n",
    "    y_train = cv_train_preds[\"true\"]\n",
    "    \n",
    "    # Get best Linear Regression model\n",
    "    best_lr = lr_models.loc[lr_models['Test_R2'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nBest LR Meta-Model:\")\n",
    "    print(f\"  Method: {best_lr['Method']}\")\n",
    "    print(f\"  Top-K: {int(best_lr['Top_K'])}\")\n",
    "    print(f\"  FS Method: {best_lr['FS_Method']}\")\n",
    "    print(f\"  Best n: {int(best_lr['Best_n'])}\")\n",
    "    print(f\"  Test R²: {best_lr['Test_R2']:.4f}\")\n",
    "    \n",
    "    # Get the top-k models used\n",
    "    top_k_models = stage1_results.sort_values(\"CV_R2\", ascending=False).head(int(best_lr['Top_K']))\n",
    "    top_k_cols = [\n",
    "        f\"pred_{row['FS_Method']}_{row['Model']}_Top{int(row['Features_Used'])}\"\n",
    "        for _, row in top_k_models.iterrows()\n",
    "    ]\n",
    "    \n",
    "    X_train_topk = X_train_stage1[top_k_cols]\n",
    "    \n",
    "    # Feature selection (same as in Stage 2)\n",
    "    if best_lr['FS_Method'] == 'RF':\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_topk, y_train)\n",
    "        scores = rf.feature_importances_\n",
    "        selected_feats = X_train_topk.columns[np.argsort(scores)[::-1][:int(best_lr['Best_n'])]]\n",
    "    elif best_lr['FS_Method'] == 'XGB':\n",
    "        from xgboost import XGBRegressor\n",
    "        xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        xgb.fit(X_train_topk, y_train)\n",
    "        scores = xgb.feature_importances_\n",
    "        selected_feats = X_train_topk.columns[np.argsort(scores)[::-1][:int(best_lr['Best_n'])]]\n",
    "    elif best_lr['FS_Method'] == 'RFE_RF':\n",
    "        from sklearn.feature_selection import RFE\n",
    "        rfe = RFE(estimator=RandomForestRegressor(random_state=42), \n",
    "                  n_features_to_select=int(best_lr['Best_n']))\n",
    "        rfe.fit(X_train_topk, y_train)\n",
    "        selected_feats = X_train_topk.columns[rfe.support_]\n",
    "    elif best_lr['FS_Method'] == 'RF_XGB':\n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_topk, y_train)\n",
    "        xgb.fit(X_train_topk, y_train)\n",
    "        avg_scores = (rf.feature_importances_ + xgb.feature_importances_) / 2\n",
    "        selected_feats = X_train_topk.columns[np.argsort(avg_scores)[::-1][:int(best_lr['Best_n'])]]\n",
    "    \n",
    "    X_train_selected = X_train_topk[selected_feats]\n",
    "    \n",
    "    # Train final Linear Regression\n",
    "    meta_model = LinearRegression()\n",
    "    meta_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Extract weights\n",
    "    weights = meta_model.coef_\n",
    "    intercept = meta_model.intercept_\n",
    "    \n",
    "    # Create weights DataFrame\n",
    "    weights_df = pd.DataFrame({\n",
    "        'Base_Model': [feat.replace('pred_', '') for feat in selected_feats],\n",
    "        'Weight': weights\n",
    "    }).sort_values('Weight', ascending=False)\n",
    "    \n",
    "    print(\"\\nMeta-Model Weights:\")\n",
    "    print(weights_df.to_string(index=False))\n",
    "    print(f\"\\nIntercept: {intercept:.4f}\")\n",
    "    print(f\"Sum of weights: {np.sum(weights):.4f}\")\n",
    "    print(f\"Dominant model: {weights_df.iloc[0]['Base_Model']} (weight={weights_df.iloc[0]['Weight']:.4f})\")\n",
    "    \n",
    "    # Save weights\n",
    "    weights_df.to_csv(os.path.join(analysis_dir, \"meta_model_weights.csv\"), index=False)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['green' if w > 0 else 'red' for w in weights_df['Weight']]\n",
    "    ax.barh(range(len(weights_df)), weights_df['Weight'], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(weights_df)))\n",
    "    ax.set_yticklabels(weights_df['Base_Model'])\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_xlabel('Weight Coefficient')\n",
    "    ax.set_title('Linear Regression Meta-Model: Base Learner Weights')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(analysis_dir, \"meta_model_weights.png\"), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved weight visualization to: {analysis_dir}/meta_model_weights.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Interpretation insights\n",
    "    print(\"\\n4.2: Interpretation Insights\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    positive_weights = weights_df[weights_df['Weight'] > 0]\n",
    "    negative_weights = weights_df[weights_df['Weight'] < 0]\n",
    "    near_zero = weights_df[abs(weights_df['Weight']) < 0.05]\n",
    "    \n",
    "    print(f\"\\nPositive contributors: {len(positive_weights)} models\")\n",
    "    print(f\"Negative contributors: {len(negative_weights)} models (bias correction)\")\n",
    "    print(f\"Near-zero weights (<0.05): {len(near_zero)} models (redundant)\")\n",
    "    \n",
    "    if len(negative_weights) > 0:\n",
    "        print(f\"\\nNegative weights suggest these models correct systematic biases:\")\n",
    "        print(negative_weights.to_string(index=False))\n",
    "    \n",
    "    if len(near_zero) > 0:\n",
    "        print(f\"\\nNear-zero weights indicate redundancy with other learners:\")\n",
    "        print(near_zero.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 5. GENERATE SUMMARY REPORT\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5] GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = []\n",
    "\n",
    "summary.append(\"=\"*80)\n",
    "summary.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n",
    "summary.append(f\"Dataset: {dataset_slug}\")\n",
    "summary.append(\"=\"*80)\n",
    "\n",
    "summary.append(\"\\n--- 1. STATISTICAL SIGNIFICANCE ---\")\n",
    "summary.append(f\"Best Stage 1 Model: {stage1_config}\")\n",
    "summary.append(f\"  Mean CV R²: {np.mean(stage1_r2_folds):.4f} ± {np.std(stage1_r2_folds, ddof=1):.4f}\")\n",
    "summary.append(f\"  Significantly better than baseline (R²=0): {'YES (p < 0.05)' if sig_df.iloc[0]['Significant'] else 'NO'}\")\n",
    "\n",
    "summary.append(f\"\\nBest Stage 2 Model: {stage2_config}\")\n",
    "summary.append(f\"  Mean CV R²: {np.mean(stage2_r2_folds):.4f} ± {np.std(stage2_r2_folds, ddof=1):.4f}\")\n",
    "summary.append(f\"  Significantly better than baseline (R²=0): {'YES (p < 0.05)' if sig_df.iloc[1]['Significant'] else 'NO'}\")\n",
    "\n",
    "if len(sig_df) > 2:\n",
    "    summary.append(f\"\\nStage 2 vs Stage 1 (Paired t-test):\")\n",
    "    summary.append(f\"  Difference in R²: {sig_df.iloc[2]['Mean_R2']:.4f}\")\n",
    "    summary.append(f\"  p-value: {sig_df.iloc[2]['p_value']:.4f}\")\n",
    "    summary.append(f\"  Significantly different: {'YES (p < 0.05)' if sig_df.iloc[2]['Significant'] else 'NO'}\")\n",
    "\n",
    "summary.append(\"\\n--- 2. STABILITY ANALYSIS ---\")\n",
    "summary.append(f\"Average CV% - Stage 1: {avg_cv_stage1:.2f}%\")\n",
    "summary.append(f\"Average CV% - Stage 2: {avg_cv_stage2:.2f}%\")\n",
    "summary.append(f\"Stability improvement: {((avg_cv_stage1 - avg_cv_stage2) / avg_cv_stage1 * 100):.1f}%\")\n",
    "\n",
    "if len(lr_models) > 0:\n",
    "    summary.append(\"\\n--- 3. INTERPRETABILITY ---\")\n",
    "    summary.append(f\"Meta-Model Type: Linear Regression\")\n",
    "    summary.append(f\"Number of base learners: {len(selected_feats)}\")\n",
    "    summary.append(f\"Dominant model: {weights_df.iloc[0]['Base_Model']} (weight={weights_df.iloc[0]['Weight']:.4f})\")\n",
    "    summary.append(f\"Positive contributors: {len(positive_weights)}\")\n",
    "    summary.append(f\"Negative contributors: {len(negative_weights)} (bias correction)\")\n",
    "    summary.append(f\"Redundant models: {len(near_zero)}\")\n",
    "\n",
    "summary_text = \"\\n\".join(summary)\n",
    "print(summary_text)\n",
    "\n",
    "with open(os.path.join(analysis_dir, \"summary_report.txt\"), 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\n✓ Saved summary report to: {analysis_dir}/summary_report.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(f\"All results saved to: {analysis_dir}/\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3516ef-ba97-41fd-9f8d-d2a8cd8b5087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
