{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ad3ebc6-297d-48fc-a020-071d9f1d7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RF with RF feature selection...\n",
      "  → HPO for RF with RF using top-5 features\n",
      "  → HPO for RF with RF using top-10 features\n",
      "  → HPO for RF with RF using top-15 features\n",
      "  → HPO for RF with RF using top-16 features\n",
      "\n",
      "Evaluating XGB with RF feature selection...\n",
      "  → HPO for XGB with RF using top-5 features\n",
      "  → HPO for XGB with RF using top-10 features\n",
      "  → HPO for XGB with RF using top-15 features\n",
      "  → HPO for XGB with RF using top-20 features\n",
      "  → HPO for XGB with RF using top-25 features\n",
      "  → HPO for XGB with RF using top-30 features\n",
      "  → HPO for XGB with RF using top-35 features\n",
      "  → HPO for XGB with RF using top-36 features\n",
      "\n",
      "Evaluating RF with RFE_RF feature selection...\n",
      "  → HPO for RF with RFE_RF using top-5 features\n",
      "  → HPO for RF with RFE_RF using top-10 features\n",
      "  → HPO for RF with RFE_RF using top-15 features\n",
      "  → HPO for RF with RFE_RF using top-20 features\n",
      "\n",
      "Evaluating XGB with RFE_RF feature selection...\n",
      "  → HPO for XGB with RFE_RF using top-5 features\n",
      "  → HPO for XGB with RFE_RF using top-10 features\n",
      "  → HPO for XGB with RFE_RF using top-15 features\n",
      "  → HPO for XGB with RFE_RF using top-20 features\n",
      "  → HPO for XGB with RFE_RF using top-25 features\n",
      "  → HPO for XGB with RFE_RF using top-30 features\n",
      "  → HPO for XGB with RFE_RF using top-35 features\n",
      "\n",
      "Evaluating RF with XGB feature selection...\n",
      "  → HPO for RF with XGB using top-5 features\n",
      "  → HPO for RF with XGB using top-10 features\n",
      "  → HPO for RF with XGB using top-14 features\n",
      "\n",
      "Evaluating XGB with XGB feature selection...\n",
      "  → HPO for XGB with XGB using top-5 features\n",
      "  → HPO for XGB with XGB using top-10 features\n",
      "  → HPO for XGB with XGB using top-15 features\n",
      "  → HPO for XGB with XGB using top-20 features\n",
      "\n",
      "Evaluating RF with RF_XGB feature selection...\n",
      "  → HPO for RF with RF_XGB using top-5 features\n",
      "  → HPO for RF with RF_XGB using top-10 features\n",
      "  → HPO for RF with RF_XGB using top-15 features\n",
      "\n",
      "Evaluating XGB with RF_XGB feature selection...\n",
      "  → HPO for XGB with RF_XGB using top-5 features\n",
      "  → HPO for XGB with RF_XGB using top-10 features\n",
      "  → HPO for XGB with RF_XGB using top-15 features\n",
      "  → HPO for XGB with RF_XGB using top-17 features\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(CONFIG[\"predictions_dir\"], exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "os.makedirs(\"hyperparams\", exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(CONFIG[\"dataset_path\"])\n",
    "    X = data.drop(\"Target\", axis=1)\n",
    "    y = data[\"Target\"]\n",
    "    return train_test_split(X, y, \n",
    "                            test_size=CONFIG[\"test_size\"],\n",
    "                            random_state=CONFIG[\"random_state\"])\n",
    "\n",
    "def get_feature_rankings(X_train, y_train):\n",
    "    rankings = {}\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "                                random_state=CONFIG[\"random_state\"])\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_scores = rf.feature_importances_\n",
    "    rankings['RF'] = list(X_train.columns[np.argsort(rf_scores)[::-1]])\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(random_state=CONFIG[\"random_state\"]),\n",
    "              n_features_to_select=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_scores = rfe.ranking_\n",
    "    rankings['RFE_RF'] = list(X_train.columns[np.argsort(rfe_scores)])\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.1,\n",
    "                       random_state=CONFIG[\"random_state\"])\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_scores = xgb.feature_importances_\n",
    "    rankings['XGB'] = list(X_train.columns[np.argsort(xgb_scores)[::-1]])\n",
    "\n",
    "    hybrid_scores = (rf_scores + xgb_scores) / 2\n",
    "    rankings['RF_XGB'] = list(X_train.columns[np.argsort(hybrid_scores)[::-1]])\n",
    "\n",
    "    return rankings\n",
    "\n",
    "def get_model_config(model_name):\n",
    "    if model_name == 'RF':\n",
    "        model_class = RandomForestRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    elif model_name == 'XGB':\n",
    "        model_class = XGBRegressor\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'random_state': [CONFIG[\"random_state\"]]\n",
    "        }\n",
    "    return model_class, param_grid\n",
    "\n",
    "def tune_hyperparameters(model, param_grid, X, y):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=CONFIG[\"metric\"],\n",
    "        cv=CONFIG[\"cv_folds\"],\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    r2 = np.max(grid_search.cv_results_[\"mean_test_score\"])\n",
    "    return {\n",
    "        \"best_model\": grid_search.best_estimator_,\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "\n",
    "def run_pipeline():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    feature_rankings = get_feature_rankings(X_train, y_train)\n",
    "    all_results = []\n",
    "    combined_preds = pd.DataFrame({'true': y_test.reset_index(drop=True)})\n",
    "    cv_train_preds = pd.DataFrame({'true': y_train.reset_index(drop=True)})\n",
    "\n",
    "    for fs_name, features in feature_rankings.items():\n",
    "        for model_name, Model in [('RF', RandomForestRegressor), ('XGB', XGBRegressor)]:\n",
    "            print(f\"\\nEvaluating {model_name} with {fs_name} feature selection...\")\n",
    "\n",
    "            best_k = None\n",
    "            best_cv_r2 = -np.inf\n",
    "\n",
    "            # Step 1: Find best k using CV\n",
    "            for k in range(1, len(features) + 1):\n",
    "                k_features = features[:k]\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                avg_r2 = np.mean(fold_r2)\n",
    "\n",
    "                if avg_r2 > best_cv_r2:\n",
    "                    best_cv_r2 = avg_r2\n",
    "                    best_k = k\n",
    "\n",
    "            # Step 2: HPO for spaced subset sizes [5, 10, ..., best_k]\n",
    "            subset_ks = [k for k in range(5, best_k + 1, 5)]\n",
    "            if best_k not in subset_ks:\n",
    "                subset_ks.append(best_k)\n",
    "\n",
    "            for k in subset_ks:\n",
    "                k_features = features[:k]\n",
    "                fold_preds = np.zeros(len(y_train))\n",
    "                fold_r2 = []\n",
    "\n",
    "                for train_idx, val_idx in KFold(CONFIG[\"cv_folds\"]).split(X_train):\n",
    "                    model = Model(random_state=CONFIG[\"random_state\"])\n",
    "                    model.fit(X_train.iloc[train_idx][k_features], y_train.iloc[train_idx])\n",
    "                    preds = model.predict(X_train.iloc[val_idx][k_features])\n",
    "                    fold_preds[val_idx] = preds\n",
    "                    fold_r2.append(r2_score(y_train.iloc[val_idx], preds))\n",
    "\n",
    "                avg_r2 = np.mean(fold_r2)\n",
    "\n",
    "                model_class, param_grid = get_model_config(model_name)\n",
    "                print(f\"  → HPO for {model_name} with {fs_name} using top-{k} features\")\n",
    "                hpo_results = tune_hyperparameters(\n",
    "                    model_class(random_state=CONFIG[\"random_state\"]),\n",
    "                    param_grid,\n",
    "                    X_train[k_features],\n",
    "                    y_train\n",
    "                )\n",
    "\n",
    "                test_preds = hpo_results[\"best_model\"].predict(X_test[k_features])\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                test_mse = mean_squared_error(y_test, test_preds)\n",
    "                test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "                model_id = f\"{fs_name}_{model_name}_Top{k}\"\n",
    "                combined_preds[f'pred_{model_id}'] = test_preds\n",
    "                cv_train_preds[f'pred_{model_id}'] = fold_preds\n",
    "\n",
    "                joblib.dump(hpo_results[\"best_model\"], f\"models/Model_{model_id}.pkl\")\n",
    "\n",
    "                with open(f\"features/Features_{model_id}.json\", 'w') as f:\n",
    "                    json.dump(k_features, f)\n",
    "\n",
    "                with open(f\"hyperparams/BestParams_{model_id}.json\", 'w') as f:\n",
    "                    json.dump(hpo_results[\"best_params\"], f)\n",
    "\n",
    "                all_results.append({\n",
    "                    'FS_Method': fs_name,\n",
    "                    'Model': model_name,\n",
    "                    'Features_Used': k,\n",
    "                    'Best_Params': hpo_results[\"best_params\"],\n",
    "                    'CV_R2': avg_r2,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Test_MSE': test_mse,\n",
    "                    'Test_MAE': test_mae\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_results)\n",
    "    metrics_df['R2_Rank'] = metrics_df['Test_R2'].rank(ascending=False, method='min')\n",
    "    metrics_df = metrics_df.sort_values('R2_Rank')\n",
    "    metrics_df.to_csv(f\"metrics/{CONFIG['results_csv_path']}\", index=False)\n",
    "    combined_preds.to_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\", index=False)\n",
    "    cv_train_preds.to_csv(f\"{CONFIG['predictions_dir']}/cv_train_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23760286-d05a-4efb-8b70-7239aae52dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MAE: 49.00236150419841\n",
      "TEST MSE: 4743.343350595725\n",
      "TEST R2 Score: 0.7756553430950164\n"
     ]
    }
   ],
   "source": [
    "stage2_test = pd.read_csv(f\"{CONFIG['predictions_dir']}/all_predictions.csv\")\n",
    "Y_test = stage2_test[\"true\"]\n",
    "X_test = stage2_test.iloc[:, 1:]\n",
    "Y_pred_test = [np.mean(X_test.iloc[i, :]) for i in range(X_test.shape[0])]\n",
    "\n",
    "mae_test = mean_absolute_error(Y_test, Y_pred_test)\n",
    "mse_test = mean_squared_error(Y_test, Y_pred_test)\n",
    "r2_test = r2_score(Y_test, Y_pred_test)\n",
    "\n",
    "print(\"TEST MAE:\" ,mae_test)\n",
    "print(\"TEST MSE:\", mse_test)\n",
    "print(\"TEST R2 Score:\", r2_test)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
